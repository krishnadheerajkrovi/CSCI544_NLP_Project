{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting emot\n",
      "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: emot\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed emot-3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import re\n",
    "# import pickle\n",
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing bad words vocab and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words=[]\n",
    "with open('./data/bad-words.csv') as f:\n",
    "    for line in f.readlines():\n",
    "        bad_words.append(line.split('\\n')[0])\n",
    "\n",
    "bad_words_set = set(bad_words)\n",
    "dataset = pd.read_csv('comments2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A lot of the clients I work with use SVB. This...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Remember when Jim Cramer a month ago said on h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The FDIC only insures up to $250k, what happen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>This is a pretty big deal , they had a huge am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>If anyone's wondering how a bank with 200+ bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65689</th>\n",
       "      <td>65689</td>\n",
       "      <td>They already have enough local staff to fill t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65690</th>\n",
       "      <td>65690</td>\n",
       "      <td>Indeed it is! It still is ridiculously below t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65691</th>\n",
       "      <td>65691</td>\n",
       "      <td>Amazon did because it's their play for DoD con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65692</th>\n",
       "      <td>65692</td>\n",
       "      <td>Thanks, I didn't get past the paywall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65693</th>\n",
       "      <td>65693</td>\n",
       "      <td>This is what happens. They build and set up, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65694 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           comments\n",
       "0               0  A lot of the clients I work with use SVB. This...\n",
       "1               1  Remember when Jim Cramer a month ago said on h...\n",
       "2               2  The FDIC only insures up to $250k, what happen...\n",
       "3               3  This is a pretty big deal , they had a huge am...\n",
       "4               4  If anyone's wondering how a bank with 200+ bil...\n",
       "...           ...                                                ...\n",
       "65689       65689  They already have enough local staff to fill t...\n",
       "65690       65690  Indeed it is! It still is ridiculously below t...\n",
       "65691       65691  Amazon did because it's their play for DoD con...\n",
       "65692       65692             Thanks, I didn't get past the paywall.\n",
       "65693       65693  This is what happens. They build and set up, g...\n",
       "\n",
       "[65694 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    return (text)\n",
    "\n",
    "def remove_contractions(text) :\n",
    "    expanded_words = []\n",
    "    expanded_text = ''\n",
    "    for word in text.split():\n",
    "        expanded_words.append(contractions.fix(word))\n",
    "        expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "remove_non_english = lambda s: re.sub(r'[^a-zA-z]', ' ', s)\n",
    "remove_spaces = lambda s: re.sub(' +',' ', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning(text):\n",
    "    #remove urls\n",
    "    text = remove_urls(text)\n",
    "    #remove html tags\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    #remove contractions \n",
    "    text = remove_contractions(text)\n",
    "    #remove non-alphabetic chars \n",
    "    text = remove_non_english(text)\n",
    "    #lowercase\n",
    "    text = text.lower( )\n",
    "    #remove extra spaces \n",
    "    text = remove_spaces(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(emot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(UNICODE_EMOJI[emot]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit()))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m----> 8\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_comments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_comments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_emojis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn [13], line 8\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      5\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(emot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(UNICODE_EMOJI[emot]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit()))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m----> 8\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_comments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_comments\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mconvert_emojis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn [13], line 5\u001b[0m, in \u001b[0;36mconvert_emojis\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_emojis\u001b[39m(text):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m emot \u001b[38;5;129;01min\u001b[39;00m UNICODE_EMOJI:\n\u001b[0;32m----> 5\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(emot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mUNICODE_EMOJI\u001b[49m\u001b[43m[\u001b[49m\u001b[43memot\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit()))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset['cleaned_comments'] = list(map(cleaning, dataset.comments))\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "dataset['cleaned_comments'] = dataset['cleaned_comments'].apply(lambda row: convert_emojis(str(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# to_remove = ['not']\n",
    "# new_stopwords = set(stopwords.words('english')).difference(to_remove)\n",
    "\n",
    "# dataset['cleaned_comments'] = dataset['cleaned_comments'].apply(lambda x: \" \".join(x for x in x.split() if x not in new_stopwords))\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# dataset['cleaned_comments'] = dataset['cleaned_comments'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Stanza POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 15:29:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca0826e50fe487d9228b06178a09274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 15:29:24 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-03-21 15:29:25 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2023-03-21 15:29:25 WARNING: GPU requested, but is not available!\n",
      "2023-03-21 15:29:25 INFO: Using device: cpu\n",
      "2023-03-21 15:29:25 INFO: Loading: tokenize\n",
      "2023-03-21 15:29:25 INFO: Loading: pos\n",
      "2023-03-21 15:29:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "pos_tagger = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos',use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tags for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = dataset[:1000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i get this feeling that derailments happen a lot and we are only paying attention now because of what happened in ohio that said why the fuck do they happen so often '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample['cleaned_comments'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_dataset_sample =[]\n",
    "for sentence in dataset_sample.iloc[:,2].values.tolist():\n",
    "    doc = pos_tagger(sentence)\n",
    "    pos_tags_dataset_sample.append([word.xpos for sent in doc.sentences for word in sent.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the tags to a txt file (Simply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pos_tags_dataset_sample.txt','w') as f:\n",
    "    f.writelines([str(i) for i in pos_tags_dataset_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tags_dataset_sample=[]\n",
    "# with open('/Users/venkatasaisumanthsadu/Documents/clg-documents/CSCI544/project/CSCI544_NLP_Project-main/pos_tags_dataset_sample.txt','r') as f:\n",
    "#     pos_tags_dataset_sample.append(f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labelling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pos_tags_dataset_sample.txt','r') as f:\n",
    "    Lines = f.readlines()\n",
    "\n",
    "    # print([tag.strip(\"'\") for tag in Lines[2].strip('\\n').split(', ')])\n",
    "    pos_tags_dataset_sample = []\n",
    "    for i in range(len(Lines)):\n",
    "        pos_tags_dataset_sample.append([tag.strip(\"'\") for tag in Lines[i].strip('\\n').split(', ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['comments'][0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['comments'][1][:].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'CC',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'VBG',\n",
       " 'IN',\n",
       " 'IN',\n",
       " 'NN']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_dataset_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample['cleaned_comments'][2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'man this whole rivalry between michigan and ohio is getting out of hand '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample['cleaned_comments'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample['pos_tag'] = pos_tags_dataset_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_comments    man this whole rivalry between michigan and ohio is getting out of hand \n",
       "labels                                                                                     0\n",
       "pos_tag                             [NN, DT, JJ, NN, IN, NNP, CC, NNP, VBZ, VBG, IN, IN, NN]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.iloc[0,2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26\n",
      "7 10\n",
      "7 16\n",
      "15 31\n",
      "16 12\n",
      "16 48\n",
      "20 26\n",
      "23 25\n",
      "25 58\n",
      "29 11\n",
      "34 35\n",
      "34 43\n",
      "34 52\n",
      "34 64\n",
      "35 10\n",
      "36 4\n",
      "39 18\n",
      "40 9\n",
      "44 0\n",
      "45 2\n",
      "47 9\n",
      "54 25\n",
      "62 12\n",
      "62 13\n",
      "67 1\n",
      "70 4\n",
      "71 8\n",
      "77 6\n",
      "79 1\n",
      "85 42\n",
      "85 48\n",
      "86 17\n",
      "87 8\n",
      "87 77\n",
      "87 108\n",
      "87 112\n",
      "87 139\n",
      "91 2\n",
      "93 2\n",
      "94 55\n",
      "100 9\n",
      "101 16\n",
      "101 17\n",
      "103 21\n",
      "103 38\n",
      "103 39\n",
      "104 25\n",
      "104 42\n",
      "105 1\n",
      "114 5\n",
      "115 30\n",
      "115 42\n",
      "118 2\n",
      "120 4\n",
      "122 13\n",
      "127 17\n",
      "132 6\n",
      "132 24\n",
      "132 52\n",
      "133 2\n",
      "136 23\n",
      "141 2\n",
      "151 10\n",
      "151 33\n",
      "159 12\n",
      "161 27\n",
      "162 40\n",
      "163 4\n",
      "165 7\n",
      "169 21\n",
      "178 14\n",
      "179 20\n",
      "179 34\n",
      "180 14\n",
      "185 1\n",
      "185 2\n",
      "187 13\n",
      "190 6\n",
      "191 3\n",
      "199 7\n",
      "199 15\n",
      "202 2\n",
      "203 0\n",
      "204 0\n",
      "208 48\n",
      "210 4\n",
      "211 5\n",
      "214 9\n",
      "216 0\n",
      "217 3\n",
      "218 66\n",
      "218 95\n",
      "219 6\n",
      "221 8\n",
      "222 0\n",
      "225 11\n",
      "228 8\n",
      "235 7\n",
      "236 8\n",
      "236 18\n",
      "236 24\n",
      "236 25\n",
      "236 30\n",
      "236 46\n",
      "236 54\n",
      "237 36\n",
      "237 45\n",
      "239 4\n",
      "241 0\n",
      "242 22\n",
      "242 23\n",
      "242 25\n",
      "242 38\n",
      "242 44\n",
      "244 13\n",
      "249 0\n",
      "252 1\n",
      "252 23\n",
      "252 59\n",
      "256 1\n",
      "256 3\n",
      "260 14\n",
      "263 2\n",
      "269 6\n",
      "269 19\n",
      "275 7\n",
      "276 27\n",
      "276 28\n",
      "278 23\n",
      "278 28\n",
      "282 38\n",
      "283 9\n",
      "287 13\n",
      "290 0\n",
      "292 13\n",
      "297 4\n",
      "297 21\n",
      "300 2\n",
      "302 2\n",
      "307 4\n",
      "308 1\n",
      "311 17\n",
      "321 12\n",
      "322 49\n",
      "328 1\n",
      "332 26\n",
      "334 12\n",
      "339 28\n",
      "340 10\n",
      "349 3\n",
      "349 11\n",
      "354 13\n",
      "355 0\n",
      "355 3\n",
      "355 6\n",
      "357 32\n",
      "358 5\n",
      "361 16\n",
      "370 1\n",
      "371 14\n",
      "373 6\n",
      "379 13\n",
      "382 37\n",
      "386 27\n",
      "386 33\n",
      "386 48\n",
      "386 60\n",
      "387 35\n",
      "394 0\n",
      "397 13\n",
      "398 46\n",
      "412 63\n",
      "416 5\n",
      "417 13\n",
      "417 15\n",
      "427 1\n",
      "429 13\n",
      "429 51\n",
      "431 51\n",
      "432 104\n",
      "434 4\n",
      "438 31\n",
      "438 43\n",
      "438 60\n",
      "438 70\n",
      "438 89\n",
      "438 90\n",
      "442 2\n",
      "442 10\n",
      "444 6\n",
      "444 15\n",
      "450 0\n",
      "450 5\n",
      "452 2\n",
      "452 5\n",
      "452 42\n",
      "452 61\n",
      "453 13\n",
      "456 14\n",
      "459 3\n",
      "460 6\n",
      "468 4\n",
      "468 10\n",
      "470 48\n",
      "475 40\n",
      "477 65\n",
      "486 0\n",
      "486 6\n",
      "486 23\n",
      "491 19\n",
      "494 12\n",
      "501 3\n",
      "501 7\n",
      "509 0\n",
      "513 12\n",
      "513 20\n",
      "520 142\n",
      "520 191\n",
      "524 13\n",
      "537 6\n",
      "541 1\n",
      "545 5\n",
      "550 10\n",
      "562 39\n",
      "563 1\n",
      "573 4\n",
      "573 15\n",
      "573 30\n",
      "573 61\n",
      "573 62\n",
      "573 79\n",
      "575 5\n",
      "575 57\n",
      "575 63\n",
      "575 78\n",
      "575 90\n",
      "576 100\n",
      "576 130\n",
      "594 2\n",
      "595 52\n",
      "595 58\n",
      "595 73\n",
      "595 85\n",
      "597 11\n",
      "601 26\n",
      "611 0\n",
      "611 35\n",
      "613 11\n",
      "614 45\n",
      "616 12\n",
      "616 18\n",
      "616 33\n",
      "616 45\n",
      "619 19\n",
      "625 8\n",
      "625 18\n",
      "626 6\n",
      "629 48\n",
      "630 20\n",
      "630 59\n",
      "635 39\n",
      "638 30\n",
      "643 1\n",
      "645 24\n",
      "645 28\n",
      "646 1\n",
      "650 5\n",
      "651 17\n",
      "651 39\n",
      "653 7\n",
      "654 10\n",
      "657 27\n",
      "658 2\n",
      "670 3\n",
      "672 115\n",
      "680 9\n",
      "683 55\n",
      "683 70\n",
      "690 10\n",
      "691 3\n",
      "693 16\n",
      "702 1\n",
      "704 9\n",
      "721 1\n",
      "724 11\n",
      "725 0\n",
      "725 1\n",
      "731 17\n",
      "732 9\n",
      "732 30\n",
      "732 45\n",
      "732 94\n",
      "735 6\n",
      "750 15\n",
      "751 17\n",
      "751 40\n",
      "756 10\n",
      "760 0\n",
      "762 4\n",
      "765 2\n",
      "765 7\n",
      "769 1\n",
      "773 21\n",
      "774 0\n",
      "782 90\n",
      "787 0\n",
      "787 16\n",
      "789 8\n",
      "790 27\n",
      "790 33\n",
      "792 43\n",
      "802 16\n",
      "802 97\n",
      "803 13\n",
      "803 63\n",
      "806 2\n",
      "811 10\n",
      "814 19\n",
      "815 30\n",
      "818 14\n",
      "823 1\n",
      "827 69\n",
      "832 15\n",
      "832 86\n",
      "833 33\n",
      "834 20\n",
      "835 4\n",
      "836 17\n",
      "850 29\n",
      "852 9\n",
      "852 12\n",
      "854 34\n",
      "856 13\n",
      "860 21\n",
      "868 15\n",
      "872 14\n",
      "874 111\n",
      "874 122\n",
      "874 180\n",
      "874 191\n",
      "876 7\n",
      "876 24\n",
      "882 19\n",
      "883 19\n",
      "889 14\n",
      "890 12\n",
      "895 6\n",
      "900 3\n",
      "903 19\n",
      "903 35\n",
      "905 180\n",
      "906 41\n",
      "908 10\n",
      "909 7\n",
      "910 20\n",
      "910 40\n",
      "911 8\n",
      "914 2\n",
      "914 30\n",
      "916 5\n",
      "916 26\n",
      "919 8\n",
      "920 32\n",
      "923 32\n",
      "923 48\n",
      "928 28\n",
      "928 30\n",
      "930 6\n",
      "930 22\n",
      "930 59\n",
      "937 17\n",
      "937 25\n",
      "937 28\n",
      "937 41\n",
      "937 48\n",
      "937 68\n",
      "942 96\n",
      "943 10\n",
      "943 16\n",
      "943 190\n",
      "946 0\n",
      "952 36\n",
      "952 114\n",
      "952 118\n",
      "961 12\n",
      "966 8\n",
      "969 14\n",
      "981 3\n",
      "982 8\n",
      "982 11\n",
      "983 20\n",
      "983 22\n",
      "987 20\n",
      "989 0\n",
      "991 13\n",
      "992 11\n",
      "994 19\n",
      "999 41\n"
     ]
    }
   ],
   "source": [
    "labels= []\n",
    "word_indices=[]\n",
    "for i,sentence in enumerate(dataset_sample['cleaned_comments']):\n",
    "    #capture bw indices for each sentence\n",
    "    flag = False\n",
    "    for j,word in enumerate(sentence.split()):\n",
    "        if word.lower() in bad_words_set:\n",
    "            flag = True\n",
    "            pos_tags_dataset_sample[i][j] = 'BW'\n",
    "            print(i,j)\n",
    "            continue\n",
    "    labels.append(1) if flag else labels.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of bad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.275"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(labels)/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating good and bad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sentences = dataset_sample[dataset_sample['labels']==0]\n",
    "bad_sentences = dataset_sample[dataset_sample['labels']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_sentences)\n",
    "good_sentences.to_csv('good_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_sentences)\n",
    "bad_sentences.to_csv('bad_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bad = bad_sentences.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comments</th>\n",
       "      <th>cleaned_comments</th>\n",
       "      <th>labels</th>\n",
       "      <th>pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>349</td>\n",
       "      <td>Is it still “conspiracy” to be skeptical that maybe some internal attack is or could be happening under situations that are always getting passed up as “statistically normal”?</td>\n",
       "      <td>is it still conspiracy to be skeptical that maybe some internal attack is or could be happening under situations that are always getting passed up as statistically normal</td>\n",
       "      <td>1</td>\n",
       "      <td>[VBZ, PRP, RB, BW, TO, VB, JJ, IN, RB, DT, JJ, BW, VBZ, CC, MD, VB, VBG, IN, NNS, WDT, VBP, RB, VBG, VBN, RP, IN, RB, JJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994</td>\n",
       "      <td>In fairness, the article has a photo in this case and it do be lookin like a \\*crash\\* crash.</td>\n",
       "      <td>in fairness the article has a photo in this case and it do be lookin like a \\ crash\\ crash</td>\n",
       "      <td>1</td>\n",
       "      <td>[IN, NN, DT, NN, VBZ, DT, NN, IN, DT, NN, CC, PRP, VBP, VB, VBG, IN, DT, HYPH, NN, BW]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>806</td>\n",
       "      <td>Am I stupid in thinking that this seems like a job a machine could do and eliminate this type of error? Yeah, I get that it would eliminate a job here but the cost of that seems much better than having what happened in Ohio in other places happen again.</td>\n",
       "      <td>am i stupid in thinking that this seems like a job a machine could do and eliminate this type of error yeah i get that it would eliminate a job here but the cost of that seems much better than having what happened in ohio in other places happen again</td>\n",
       "      <td>1</td>\n",
       "      <td>[VBP, PRP, BW, IN, VBG, IN, DT, VBZ, IN, DT, NN, DT, NN, MD, VB, CC, VB, DT, NN, IN, NN, UH, PRP, VBP, IN, PRP, MD, VB, DT, NN, RB, CC, DT, NN, IN, DT, VBZ, RB, JJR, IN, VBG, WP, VBD, IN, NNP, IN, JJ, NNS, VB, RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>658</td>\n",
       "      <td>America finished fucking around and now we are finding out.</td>\n",
       "      <td>america finished fucking around and now we are finding out</td>\n",
       "      <td>1</td>\n",
       "      <td>[NNP, VBD, BW, RB, CC, RB, PRP, VBP, VBG, RP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>427</td>\n",
       "      <td>Oh God first the giant spy balloons now the hazmat train derailments, what else have they been keeping from us...</td>\n",
       "      <td>oh god first the giant spy balloons now the hazmat train derailments what else have they been keeping from us</td>\n",
       "      <td>1</td>\n",
       "      <td>[UH, BW, RB, DT, JJ, NN, NNS, RB, DT, NN, NN, VBZ, WP, RB, VBP, PRP, VBN, VBG, IN, PRP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>Maybe we should give the workers paid sick leave</td>\n",
       "      <td>maybe we should give the workers paid sick leave</td>\n",
       "      <td>1</td>\n",
       "      <td>[RB, PRP, MD, VB, DT, NNS, VBN, BW, NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>322</td>\n",
       "      <td>So.....I'm starting to get a funny feeling I'm causing these. Due to some concerns we are having my family is debating moving to a lower COL locations where we can find work...Those options and the leading options were Ohio....then derailment...And...Michigan. Are you fucking kidding me?Based on this. Minnesota and Texas have derailments, I'm pretty sure I'm a super villain guys.</td>\n",
       "      <td>so i am starting to get a funny feeling i am causing these due to some concerns we are having my family is debating moving to a lower col locations where we can find work those options and the leading options were ohio then derailment and michigan are you fucking kidding me based on this minnesota and texas have derailments i am pretty sure i am a super villain guys</td>\n",
       "      <td>1</td>\n",
       "      <td>[RB, PRP, VBP, VBG, TO, VB, DT, JJ, NN, PRP, VBP, VBG, DT, IN, IN, DT, NNS, PRP, VBP, VBG, PRP$, NN, VBZ, VBG, VBG, IN, DT, JJR, NN, NNS, WRB, PRP, MD, VB, NN, DT, NNS, CC, DT, VBG, NNS, VBD, NNP, RB, NN, CC, NNP, VBP, PRP, BW, VBG, PRP, VBN, IN, DT, NNP, CC, NNP, VBP, NNS, PRP, VBP, RB, JJ, PRP, VBP, DT, JJ, NN, NNS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>180</td>\n",
       "      <td>This is what happens when Republicans save their rich buddies a fortune while gutting American Infrastructure . This is the \"America First\" party as they call themselves .</td>\n",
       "      <td>this is what happens when republicans save their rich buddies a fortune while gutting american infrastructure this is the america first party as they call themselves</td>\n",
       "      <td>1</td>\n",
       "      <td>[DT, VBZ, WP, VBZ, WRB, NNPS, VBP, PRP$, JJ, NNS, DT, NN, IN, VBG, BW, NN, DT, VBZ, DT, NNP, JJ, NN, IN, PRP, VBP, PRP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237</td>\n",
       "      <td>Aussie here, but I keep hearing how Biden reduced the money required to maintain the rail network in America? Is this true? Sounds similar to how our former prime minister reduced the money for pre bush fire season and half of Auatralia was basically on fire.</td>\n",
       "      <td>aussie here but i keep hearing how biden reduced the money required to maintain the rail network in america is this true sounds similar to how our former prime minister reduced the money for pre bush fire season and half of auatralia was basically on fire</td>\n",
       "      <td>1</td>\n",
       "      <td>[NN, RB, CC, PRP, VBP, VBG, WRB, NNP, VBD, DT, NN, VBN, TO, VB, DT, NN, NN, IN, NNP, VBZ, DT, JJ, VBZ, JJ, IN, WRB, PRP$, JJ, JJ, NN, VBD, DT, NN, IN, NN, NNP, BW, NN, CC, NN, IN, NN, VBD, RB, IN, BW]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>382</td>\n",
       "      <td>[This article](https://www.independent.co.uk/news/world/americas/train-derailments-in-2023-how-many-b2283418.html) has this metric:&gt; The Bureau of Transportation Statistics records 54,539 train derailments between 1990 to 2021, an average of 1,704 per year.Also includes:&gt; Rail experts have argued, however, that the East Palestine crash was the inevitable result of compromised safety measures and reduced workforces, part of an effort to boost rail company profits.</td>\n",
       "      <td>[this article] derailments in how many b html has this metric the bureau of transportation statistics records train derailments between to an average of per year also includes rail experts have argued however that the east palestine crash was the inevitable result of compromised safety measures and reduced workforces part of an effort to boost rail company profits</td>\n",
       "      <td>1</td>\n",
       "      <td>[-LRB-, DT, NN, -RRB-, VBZ, IN, WRB, JJ, NN, NN, VBZ, DT, NN, DT, NN, IN, NN, NNS, VBZ, NN, NNS, IN, IN, DT, NN, IN, IN, NN, RB, VBZ, NN, NNS, VBP, VBN, RB, IN, DT, BW, NNP, NN, VBD, DT, JJ, NN, IN, VBN, NN, NNS, CC, VBN, NNS, NN, IN, DT, NN, TO, VB, NN, NN, NNS]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  \\\n",
       "349         349   \n",
       "994         994   \n",
       "806         806   \n",
       "658         658   \n",
       "427         427   \n",
       "235         235   \n",
       "322         322   \n",
       "180         180   \n",
       "237         237   \n",
       "382         382   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                comments  \\\n",
       "349                                                                                                                                                                                                                                                                                                      Is it still “conspiracy” to be skeptical that maybe some internal attack is or could be happening under situations that are always getting passed up as “statistically normal”?   \n",
       "994                                                                                                                                                                                                                                                                                                                                                                                        In fairness, the article has a photo in this case and it do be lookin like a \\*crash\\* crash.   \n",
       "806                                                                                                                                                                                                                        Am I stupid in thinking that this seems like a job a machine could do and eliminate this type of error? Yeah, I get that it would eliminate a job here but the cost of that seems much better than having what happened in Ohio in other places happen again.   \n",
       "658                                                                                                                                                                                                                                                                                                                                                                                                                          America finished fucking around and now we are finding out.   \n",
       "427                                                                                                                                                                                                                                                                                                                                                                    Oh God first the giant spy balloons now the hazmat train derailments, what else have they been keeping from us...   \n",
       "235                                                                                                                                                                                                                                                                                                                                                                                                                                     Maybe we should give the workers paid sick leave   \n",
       "322                                                                                       So.....I'm starting to get a funny feeling I'm causing these. Due to some concerns we are having my family is debating moving to a lower COL locations where we can find work...Those options and the leading options were Ohio....then derailment...And...Michigan. Are you fucking kidding me?Based on this. Minnesota and Texas have derailments, I'm pretty sure I'm a super villain guys.   \n",
       "180                                                                                                                                                                                                                                                                                                          This is what happens when Republicans save their rich buddies a fortune while gutting American Infrastructure . This is the \"America First\" party as they call themselves .   \n",
       "237                                                                                                                                                                                                                  Aussie here, but I keep hearing how Biden reduced the money required to maintain the rail network in America? Is this true? Sounds similar to how our former prime minister reduced the money for pre bush fire season and half of Auatralia was basically on fire.   \n",
       "382  [This article](https://www.independent.co.uk/news/world/americas/train-derailments-in-2023-how-many-b2283418.html) has this metric:> The Bureau of Transportation Statistics records 54,539 train derailments between 1990 to 2021, an average of 1,704 per year.Also includes:> Rail experts have argued, however, that the East Palestine crash was the inevitable result of compromised safety measures and reduced workforces, part of an effort to boost rail company profits.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                      cleaned_comments  \\\n",
       "349                                                                                                                                                                                                        is it still conspiracy to be skeptical that maybe some internal attack is or could be happening under situations that are always getting passed up as statistically normal    \n",
       "994                                                                                                                                                                                                                                                                                        in fairness the article has a photo in this case and it do be lookin like a \\ crash\\ crash    \n",
       "806                                                                                                                        am i stupid in thinking that this seems like a job a machine could do and eliminate this type of error yeah i get that it would eliminate a job here but the cost of that seems much better than having what happened in ohio in other places happen again    \n",
       "658                                                                                                                                                                                                                                                                                                                        america finished fucking around and now we are finding out    \n",
       "427                                                                                                                                                                                                                                                                     oh god first the giant spy balloons now the hazmat train derailments what else have they been keeping from us    \n",
       "235                                                                                                                                                                                                                                                                                                                                   maybe we should give the workers paid sick leave   \n",
       "322  so i am starting to get a funny feeling i am causing these due to some concerns we are having my family is debating moving to a lower col locations where we can find work those options and the leading options were ohio then derailment and michigan are you fucking kidding me based on this minnesota and texas have derailments i am pretty sure i am a super villain guys    \n",
       "180                                                                                                                                                                                                             this is what happens when republicans save their rich buddies a fortune while gutting american infrastructure this is the america first party as they call themselves    \n",
       "237                                                                                                                   aussie here but i keep hearing how biden reduced the money required to maintain the rail network in america is this true sounds similar to how our former prime minister reduced the money for pre bush fire season and half of auatralia was basically on fire    \n",
       "382    [this article] derailments in how many b html has this metric the bureau of transportation statistics records train derailments between to an average of per year also includes rail experts have argued however that the east palestine crash was the inevitable result of compromised safety measures and reduced workforces part of an effort to boost rail company profits    \n",
       "\n",
       "     labels  \\\n",
       "349       1   \n",
       "994       1   \n",
       "806       1   \n",
       "658       1   \n",
       "427       1   \n",
       "235       1   \n",
       "322       1   \n",
       "180       1   \n",
       "237       1   \n",
       "382       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                             pos_tag  \n",
       "349                                                                                                                                                                                                        [VBZ, PRP, RB, BW, TO, VB, JJ, IN, RB, DT, JJ, BW, VBZ, CC, MD, VB, VBG, IN, NNS, WDT, VBP, RB, VBG, VBN, RP, IN, RB, JJ]  \n",
       "994                                                                                                                                                                                                                                           [IN, NN, DT, NN, VBZ, DT, NN, IN, DT, NN, CC, PRP, VBP, VB, VBG, IN, DT, HYPH, NN, BW]  \n",
       "806                                                                                                            [VBP, PRP, BW, IN, VBG, IN, DT, VBZ, IN, DT, NN, DT, NN, MD, VB, CC, VB, DT, NN, IN, NN, UH, PRP, VBP, IN, PRP, MD, VB, DT, NN, RB, CC, DT, NN, IN, DT, VBZ, RB, JJR, IN, VBG, WP, VBD, IN, NNP, IN, JJ, NNS, VB, RB]  \n",
       "658                                                                                                                                                                                                                                                                                    [NNP, VBD, BW, RB, CC, RB, PRP, VBP, VBG, RP]  \n",
       "427                                                                                                                                                                                                                                          [UH, BW, RB, DT, JJ, NN, NNS, RB, DT, NN, NN, VBZ, WP, RB, VBP, PRP, VBN, VBG, IN, PRP]  \n",
       "235                                                                                                                                                                                                                                                                                          [RB, PRP, MD, VB, DT, NNS, VBN, BW, NN]  \n",
       "322  [RB, PRP, VBP, VBG, TO, VB, DT, JJ, NN, PRP, VBP, VBG, DT, IN, IN, DT, NNS, PRP, VBP, VBG, PRP$, NN, VBZ, VBG, VBG, IN, DT, JJR, NN, NNS, WRB, PRP, MD, VB, NN, DT, NNS, CC, DT, VBG, NNS, VBD, NNP, RB, NN, CC, NNP, VBP, PRP, BW, VBG, PRP, VBN, IN, DT, NNP, CC, NNP, VBP, NNS, PRP, VBP, RB, JJ, PRP, VBP, DT, JJ, NN, NNS]  \n",
       "180                                                                                                                                                                                                          [DT, VBZ, WP, VBZ, WRB, NNPS, VBP, PRP$, JJ, NNS, DT, NN, IN, VBG, BW, NN, DT, VBZ, DT, NNP, JJ, NN, IN, PRP, VBP, PRP]  \n",
       "237                                                                                                                         [NN, RB, CC, PRP, VBP, VBG, WRB, NNP, VBD, DT, NN, VBN, TO, VB, DT, NN, NN, IN, NNP, VBZ, DT, JJ, VBZ, JJ, IN, WRB, PRP$, JJ, JJ, NN, VBD, DT, NN, IN, NN, NNP, BW, NN, CC, NN, IN, NN, VBD, RB, IN, BW]  \n",
       "382                                                          [-LRB-, DT, NN, -RRB-, VBZ, IN, WRB, JJ, NN, NN, VBZ, DT, NN, DT, NN, IN, NN, NNS, VBZ, NN, NNS, IN, IN, DT, NN, IN, IN, NN, RB, VBZ, NN, NNS, VBP, VBN, RB, IN, DT, BW, NNP, NN, VBD, DT, JJ, NN, IN, VBN, NN, NNS, CC, VBN, NNS, NN, IN, DT, NN, TO, VB, NN, NN, NNS]  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lucene\n",
    "# from java.io import StringReader\n",
    "# from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "# from org.apache.lucene.index import DirectoryReader\n",
    "# from org.apache.lucene.search.similarities import ClassicSimilarity\n",
    "# from org.apache.lucene.search import IndexSearcher\n",
    "# from org.apache.lucene.queryparser.classic import QueryParser\n",
    "# from org.apache.lucene.store import SimpleFSDirectory\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Initialize Lucene\n",
    "# lucene.initVM()\n",
    "\n",
    "# # Set up the analyzer and similarity algorithm\n",
    "# analyzer = StandardAnalyzer()\n",
    "# similarity = ClassicSimilarity()\n",
    "\n",
    "# # Set up the index\n",
    "# index_dir = SimpleFSDirectory(File(\"index\"))\n",
    "# searcher = IndexSearcher(DirectoryReader.open(index_dir))\n",
    "# searcher.setSimilarity(similarity)\n",
    "\n",
    "# # Define the query and tags\n",
    "# query = \"python programming\"\n",
    "# tags = [\"python\", \"programming\"]\n",
    "\n",
    "# # Tokenize and vectorize the tags using TF-IDF\n",
    "# tfidf = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "# tfidf_matrix = tfidf.fit_transform(tags)\n",
    "\n",
    "# # Search the index for similar sentences\n",
    "# query_parser = QueryParser(\"content\", analyzer)\n",
    "# query = query_parser.parse(query)\n",
    "# top_docs = searcher.search(query, 10)\n",
    "# for score_doc in top_docs.scoreDocs:\n",
    "#     doc = searcher.doc(score_doc.doc)\n",
    "#     sentence = doc.get(\"content\")\n",
    "#     tfidf_score = cosine_similarity(tfidf_matrix, tfidf.transform([sentence]))[0][0]\n",
    "#     if tfidf_score > 0.5:\n",
    "#         print(f\"Similar sentence found: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.25861529 0.25861529]\n",
      " [0.25861529 1.         0.25861529]\n",
      " [0.25861529 0.25861529 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the sentences\n",
    "sentences = [\n",
    "    # \"The quick brown cat jumps over the lazy dog\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute the tf-idf matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Print the similarity matrix\n",
    "print(cosine_sim_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'VBZ', 'VBG', 'IN', 'IN', 'NN', '.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with open('pos_tags_dataset_duplicate.txt','r') as f:\n",
    "#     Lines = f.readlines()\n",
    "\n",
    "#     # print(Lines[0])\n",
    "#     lst = []\n",
    "#     for i in range(len(Lines)):\n",
    "#         lst.append(list(Lines[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": "20",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e354bad413bab38fe3bbeda77f9d500bf301a7ca6b46dcc1d81c4ff022f2634c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
