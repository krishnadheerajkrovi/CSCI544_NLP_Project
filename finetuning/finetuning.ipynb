{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel,BertForMaskedLM\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_comments = pd.read_csv('data/pos_tags_dataset.csv')\n",
    "input_sentences = labeled_comments['cleaned_comments'].tolist()\n",
    "input_sentences = [sentence for sentence in input_sentences if type(sentence)==str and  len(sentence.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bad_word.txt', 'r') as f:\n",
    "    not_predict = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 100/100 [00:15<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 8.4241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.4234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.1480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 0.1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 0.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 0.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss: 0.0614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define a list of words that the model should not predict\n",
    "\n",
    "# Load a pre-trained BERT model and tokenizer\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model = model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the optimizer and loss function for fine-tuning\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define a list of input sentences and labels\n",
    "sentences = input_sentences[:100]\n",
    "\n",
    "labels = []\n",
    "for sentence in sentences:\n",
    "    label_sentence = []\n",
    "    try:\n",
    "        if len(sentence)>1:\n",
    "            sentence = sentence.split()\n",
    "        for word in sentence:\n",
    "            if word in not_predict:\n",
    "                label_sentence.append(0)\n",
    "            else:\n",
    "                label_sentence.append(1)\n",
    "        labels.append(label_sentence)\n",
    "    except:\n",
    "        print(sentence)\n",
    "\n",
    "\n",
    "# Convert the input sentences to input features\n",
    "input_features = []\n",
    "for sentence in sentences:\n",
    "    # Tokenize the input sentence\n",
    "    tokenized_sentence = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    # Create a list of labels for each token in the input sentence\n",
    "    sentence_labels = []\n",
    "    for token in tokenized_sentence:\n",
    "        word = tokenizer.decode([token]).strip()\n",
    "        if word in not_predict:\n",
    "            sentence_labels.append(0)  # label 0 for blacklisted words\n",
    "        else:\n",
    "            sentence_labels.append(1)  # label 1 for other words\n",
    "    # Append the input features and labels to the input_features list\n",
    "    input_features.append((torch.tensor(tokenized_sentence).to(device), torch.tensor(sentence_labels).to(device)))\n",
    "\n",
    "# Fine-tune the BERT model on the input features and labels\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for input_ids, labels in tqdm(input_features):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids.unsqueeze(0), labels=labels.unsqueeze(0))\n",
    "        loss, prediction_scores = outputs[:2]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch {} loss: {:.4f}\".format(epoch+1, running_loss/len(input_features)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  2182,  3310,  1996,  2927,  1997,  2178,  8239, 19396,   102],\n",
      "       device='cuda:0')\n",
      "MaskedLMOutput(loss=None, logits=tensor([[[15.3306, 19.9276,  8.0185,  ...,  7.0004,  4.2338,  3.4028],\n",
      "         [15.1143, 19.7127,  7.7288,  ...,  6.8165,  4.2824,  3.0219],\n",
      "         [15.0954, 19.6774,  7.7642,  ...,  6.8457,  4.3665,  3.0776],\n",
      "         ...,\n",
      "         [15.2913, 19.8027,  7.8640,  ...,  6.9566,  4.3831,  3.1447],\n",
      "         [15.4877, 20.0838,  8.1257,  ...,  7.0895,  4.6768,  3.3599],\n",
      "         [14.0324, 18.7651,  6.8386,  ...,  6.1382,  3.7032,  2.3645]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Input sentence: here comes the beginning of another fucking recession\n",
      "Predicted words: ['[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]']\n",
      "tensor([  101,  2002, 15646,  1037,  8239,  4589, 10869,  1012,   102],\n",
      "       device='cuda:0')\n",
      "MaskedLMOutput(loss=None, logits=tensor([[[14.6580, 19.4187,  7.5962,  ...,  6.7878,  3.8052,  3.8945],\n",
      "         [14.6751, 19.3936,  7.4624,  ...,  6.6313,  4.0498,  3.4757],\n",
      "         [15.1141, 19.7990,  7.8611,  ...,  6.8948,  4.3322,  3.7253],\n",
      "         ...,\n",
      "         [15.0091, 19.7011,  7.7563,  ...,  6.8666,  4.4595,  3.8075],\n",
      "         [14.6008, 19.2695,  7.3239,  ...,  6.3845,  4.1873,  3.5765],\n",
      "         [13.9999, 18.7947,  6.8867,  ...,  6.2552,  3.8707,  3.0423]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Input sentence: he enjoys a fucking orange juice.\n",
      "Predicted words: ['[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]']\n",
      "tensor([  101,  2016,  4149,  2070, 28124,  2012,  1996,  3573,  1012,   102],\n",
      "       device='cuda:0')\n",
      "MaskedLMOutput(loss=None, logits=tensor([[[14.4178, 19.2180,  7.4740,  ...,  6.3044,  3.5469,  3.1269],\n",
      "         [14.8783, 19.5937,  7.8334,  ...,  6.5667,  4.3491,  3.6113],\n",
      "         [14.8229, 19.5490,  7.7140,  ...,  6.3990,  4.1937,  3.2658],\n",
      "         ...,\n",
      "         [15.1305, 19.8484,  8.1072,  ...,  6.8703,  4.5682,  3.5282],\n",
      "         [14.1797, 18.8866,  7.0347,  ...,  5.8291,  3.8420,  3.0426],\n",
      "         [13.9314, 18.7178,  6.9650,  ...,  6.0292,  3.8929,  2.5687]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Input sentence: she bought some nipples at the store.\n",
      "Predicted words: ['[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]']\n",
      "tensor([  101,  2057,  2253,  2000,  1996,  2380,  2000,  2377, 26476,  1012,\n",
      "          102], device='cuda:0')\n",
      "MaskedLMOutput(loss=None, logits=tensor([[[13.7878, 18.6534,  6.7157,  ...,  5.7745,  3.0234,  3.2597],\n",
      "         [13.9140, 18.7420,  6.7661,  ...,  5.7691,  3.4886,  2.9077],\n",
      "         [13.6453, 18.4818,  6.4408,  ...,  5.4809,  3.2467,  2.5772],\n",
      "         ...,\n",
      "         [14.5912, 19.3682,  7.3927,  ...,  6.2826,  4.1170,  3.3781],\n",
      "         [13.6103, 18.4150,  6.3780,  ...,  5.3789,  3.5239,  3.0233],\n",
      "         [13.0858, 17.9237,  6.0493,  ...,  5.2628,  3.2758,  2.3415]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Input sentence: we went to the park to play sucker.\n",
      "Predicted words: ['[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]']\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on some example input sentences\n",
    "test_sentences = [\n",
    "    \"here comes the beginning of another fucking recession\",\n",
    "    \"he enjoys a fucking orange juice.\",\n",
    "    \"she bought some nipples at the store.\",\n",
    "    \"we went to the park to play sucker.\"\n",
    "]\n",
    "for sentence in test_sentences:\n",
    "    # Tokenize the input sentence\n",
    "    tokenized_sentence = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    # Create a list of labels for each token in the input sentence\n",
    "    sentence_labels = []\n",
    "    # Convert the input sentence to a PyTorch tensor and predict the masked words\n",
    "    input_ids = torch.tensor(tokenized_sentence).to(device)\n",
    "    print(input_ids)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.unsqueeze(0))\n",
    "        print(outputs)\n",
    "        prediction_scores = outputs[0]\n",
    "    # Decode the predicted masked words and print the result\n",
    "    predicted_tokens = torch.argmax(prediction_scores, dim=-1).squeeze().tolist()\n",
    "    print(predicted_tokens)\n",
    "    predicted_words = tokenizer.decode(predicted_tokens).split()\n",
    "    print(\"Input sentence:\", sentence)\n",
    "    print(\"Predicted words:\", predicted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci544",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
