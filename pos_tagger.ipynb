{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/venkatasaisumanthsadu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/venkatasaisumanthsadu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/venkatasaisumanthsadu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/venkatasaisumanthsadu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import re\n",
    "# import pickle\n",
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing bad words vocab and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words=[]\n",
    "with open('./data/bad-words.csv') as f:\n",
    "    for line in f.readlines():\n",
    "        bad_words.append(line.split('\\n')[0])\n",
    "\n",
    "bad_words_set = set(bad_words)\n",
    "dataset = pd.read_csv('news_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Man this whole rivalry between Michigan and Oh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I get this feeling that derailments happen A L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>They happen a lot. I work in shipping and use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Weird that when we don't invest in infrastruct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Well, they've clearly learned from East Palest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51698</th>\n",
       "      <td>51698</td>\n",
       "      <td>Ukraine also has the average of superior train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51699</th>\n",
       "      <td>51699</td>\n",
       "      <td>Does NK make their own shells? Or was Russia b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51700</th>\n",
       "      <td>51700</td>\n",
       "      <td>counterpoint: if there's one thing Best Korea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51701</th>\n",
       "      <td>51701</td>\n",
       "      <td>American intelligence is absolutely crucial to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51702</th>\n",
       "      <td>51702</td>\n",
       "      <td>They were trying to buy some of the surplus So...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51703 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           comments\n",
       "0               0  Man this whole rivalry between Michigan and Oh...\n",
       "1               1  I get this feeling that derailments happen A L...\n",
       "2               2  They happen a lot. I work in shipping and use ...\n",
       "3               3  Weird that when we don't invest in infrastruct...\n",
       "4               4  Well, they've clearly learned from East Palest...\n",
       "...           ...                                                ...\n",
       "51698       51698  Ukraine also has the average of superior train...\n",
       "51699       51699  Does NK make their own shells? Or was Russia b...\n",
       "51700       51700  counterpoint: if there's one thing Best Korea ...\n",
       "51701       51701  American intelligence is absolutely crucial to...\n",
       "51702       51702  They were trying to buy some of the surplus So...\n",
       "\n",
       "[51703 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    return (text)\n",
    "\n",
    "# def remove_contractions(text) :\n",
    "#     expanded_words = []\n",
    "#     for word in text.split():\n",
    "#         expanded_words.append(contractions.fix(word))\n",
    "#         expanded_text = ' '.join(expanded_words)\n",
    "#     return expanded_text\n",
    "\n",
    "\n",
    "remove_non_english = lambda s: re.sub(r'[^a-zA-z]', ' ', s)\n",
    "remove_spaces = lambda s: re.sub(' +',' ', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning(text):\n",
    "    #remove urls\n",
    "    text = remove_urls(text)\n",
    "    #remove html tags\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    #remove contractions \n",
    "    # text = remove_contractions(text)\n",
    "    #remove non-alphabetic chars \n",
    "    text = remove_non_english(text)\n",
    "    #lowercase\n",
    "    text = text.lower( )\n",
    "    #remove extra spaces \n",
    "    text = remove_spaces(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['cleaned_comments'] = list(map(cleaning, dataset.comments))\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "dataset['cleaned_comments'] = dataset['cleaned_comments'].apply(lambda row: convert_emojis(str(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# to_remove = ['not']\n",
    "# new_stopwords = set(stopwords.words('english')).difference(to_remove)\n",
    "\n",
    "# dataset['cleaned_comments'] = dataset['cleaned_comments'].apply(lambda x: \" \".join(x for x in x.split() if x not in new_stopwords))\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# dataset['cleaned_comments'] = dataset['cleaned_comments'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Stanza POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 23:55:23 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8154793b7fb948af9c0f7bc6de83f32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 23:55:24 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-03-20 23:55:24 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2023-03-20 23:55:24 WARNING: GPU requested, but is not available!\n",
      "2023-03-20 23:55:24 INFO: Using device: cpu\n",
      "2023-03-20 23:55:24 INFO: Loading: tokenize\n",
      "2023-03-20 23:55:24 INFO: Loading: pos\n",
      "2023-03-20 23:55:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "pos_tagger = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos',use_gpu=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tags for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = dataset[:50].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i get this feeling that derailments happen a lot and we re only paying attention now because of what happened in ohio that said why the fuck do they happen so often '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample['cleaned_comments'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_dataset_sample =[]\n",
    "for sentence in dataset_sample.iloc[:,2].values.tolist():\n",
    "    doc = pos_tagger(sentence)\n",
    "    pos_tags_dataset_sample.append([word.xpos for sent in doc.sentences for word in sent.words])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the tags to a txt file (Simply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pos_tags_dataset_sample.txt','w') as f:\n",
    "    f.writelines([str(i) for i in pos_tags_dataset_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tags_dataset_sample=[]\n",
    "# with open('/Users/venkatasaisumanthsadu/Documents/clg-documents/CSCI544/project/CSCI544_NLP_Project-main/pos_tags_dataset_sample.txt','r') as f:\n",
    "#     pos_tags_dataset_sample.append(f.readlines())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labelling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pos_tags_dataset_sample.txt','r') as f:\n",
    "    Lines = f.readlines()\n",
    "\n",
    "    # print([tag.strip(\"'\") for tag in Lines[2].strip('\\n').split(', ')])\n",
    "    pos_tags_dataset_sample = []\n",
    "    for i in range(len(Lines)):\n",
    "        pos_tags_dataset_sample.append([tag.strip(\"'\") for tag in Lines[i].strip('\\n').split(', ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['comments'][0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['comments'][1][:].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRP',\n",
       " 'VBP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'VBP',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'CC',\n",
       " 'PRP',\n",
       " 'VBZ',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'PRP',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'JJ',\n",
       " 'VBG',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_dataset_sample[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRP',\n",
       " 'VBP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'VBP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'RB',\n",
       " 'VBG',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'IN',\n",
       " 'IN',\n",
       " 'WP',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'WDT',\n",
       " 'VBD',\n",
       " 'WRB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBP',\n",
       " 'PRP',\n",
       " 'VB',\n",
       " 'RB',\n",
       " 'RB']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_dataset_sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample['cleaned_comments'][2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'man this whole rivalry between michigan and ohio is getting out of hand '"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample['cleaned_comments'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26\n",
      "7 10\n",
      "7 16\n",
      "15 31\n",
      "16 12\n",
      "16 48\n",
      "20 26\n",
      "23 25\n",
      "25 58\n",
      "29 11\n",
      "34 35\n",
      "34 43\n",
      "34 52\n",
      "34 64\n",
      "35 10\n",
      "36 4\n",
      "39 18\n",
      "40 9\n",
      "44 0\n",
      "45 2\n",
      "47 9\n"
     ]
    }
   ],
   "source": [
    "labels= []\n",
    "word_indices=[]\n",
    "for i,sentence in enumerate(dataset_sample['cleaned_comments']):\n",
    "    #capture bw indices for each sentence\n",
    "    flag = False\n",
    "    for j,word in enumerate(sentence.split()):\n",
    "        if word.lower() in bad_words_set:\n",
    "            flag = True\n",
    "            pos_tags_dataset_sample[i][j] = 'BW'\n",
    "            print(i,j)\n",
    "            continue\n",
    "    labels.append(1) if flag else labels.append(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of bad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14439"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27926812757480224"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(labels)/len(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating good and bad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sentences = dataset[dataset['labels']==0]\n",
    "bad_sentences = dataset[dataset['labels']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_sentences)\n",
    "good_sentences.to_csv('good_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_sentences)\n",
    "bad_sentences.to_csv('bad_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bad = bad_sentences.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33865</th>\n",
       "      <td>33865</td>\n",
       "      <td>Basically what I'm saying is that if they clai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15899</th>\n",
       "      <td>15899</td>\n",
       "      <td>The Ferenghi do have a general reservation aga...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24288</th>\n",
       "      <td>24288</td>\n",
       "      <td>PPP Fraud helped business owners buy all their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50279</th>\n",
       "      <td>50279</td>\n",
       "      <td>These are a great alternative to abandoning yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16376</th>\n",
       "      <td>16376</td>\n",
       "      <td>No vinyl chloride or pre-product has been dete...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33306</th>\n",
       "      <td>33306</td>\n",
       "      <td>What the fuck, you are regularly finding dead ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24605</th>\n",
       "      <td>24605</td>\n",
       "      <td>*If* he's being sarcastic, fascists managed to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44891</th>\n",
       "      <td>44891</td>\n",
       "      <td>It'd be terrifying as fuck when you awake and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12769</th>\n",
       "      <td>12769</td>\n",
       "      <td>&gt; Gaetzâ€™s father is famous for fixing things f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46345</th>\n",
       "      <td>46345</td>\n",
       "      <td>Union Carbide killed over 3700 people and inju...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           comments  labels\n",
       "33865       33865  Basically what I'm saying is that if they clai...       1\n",
       "15899       15899  The Ferenghi do have a general reservation aga...       1\n",
       "24288       24288  PPP Fraud helped business owners buy all their...       1\n",
       "50279       50279  These are a great alternative to abandoning yo...       1\n",
       "16376       16376  No vinyl chloride or pre-product has been dete...       1\n",
       "33306       33306  What the fuck, you are regularly finding dead ...       1\n",
       "24605       24605  *If* he's being sarcastic, fascists managed to...       1\n",
       "44891       44891  It'd be terrifying as fuck when you awake and ...       1\n",
       "12769       12769  > Gaetzâ€™s father is famous for fixing things f...       1\n",
       "46345       46345  Union Carbide killed over 3700 people and inju...       1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lucene\n",
    "# from java.io import StringReader\n",
    "# from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "# from org.apache.lucene.index import DirectoryReader\n",
    "# from org.apache.lucene.search.similarities import ClassicSimilarity\n",
    "# from org.apache.lucene.search import IndexSearcher\n",
    "# from org.apache.lucene.queryparser.classic import QueryParser\n",
    "# from org.apache.lucene.store import SimpleFSDirectory\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Initialize Lucene\n",
    "# lucene.initVM()\n",
    "\n",
    "# # Set up the analyzer and similarity algorithm\n",
    "# analyzer = StandardAnalyzer()\n",
    "# similarity = ClassicSimilarity()\n",
    "\n",
    "# # Set up the index\n",
    "# index_dir = SimpleFSDirectory(File(\"index\"))\n",
    "# searcher = IndexSearcher(DirectoryReader.open(index_dir))\n",
    "# searcher.setSimilarity(similarity)\n",
    "\n",
    "# # Define the query and tags\n",
    "# query = \"python programming\"\n",
    "# tags = [\"python\", \"programming\"]\n",
    "\n",
    "# # Tokenize and vectorize the tags using TF-IDF\n",
    "# tfidf = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "# tfidf_matrix = tfidf.fit_transform(tags)\n",
    "\n",
    "# # Search the index for similar sentences\n",
    "# query_parser = QueryParser(\"content\", analyzer)\n",
    "# query = query_parser.parse(query)\n",
    "# top_docs = searcher.search(query, 10)\n",
    "# for score_doc in top_docs.scoreDocs:\n",
    "#     doc = searcher.doc(score_doc.doc)\n",
    "#     sentence = doc.get(\"content\")\n",
    "#     tfidf_score = cosine_similarity(tfidf_matrix, tfidf.transform([sentence]))[0][0]\n",
    "#     if tfidf_score > 0.5:\n",
    "#         print(f\"Similar sentence found: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.25861529 0.25861529]\n",
      " [0.25861529 1.         0.25861529]\n",
      " [0.25861529 0.25861529 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the sentences\n",
    "sentences = [\n",
    "    # \"The quick brown cat jumps over the lazy dog\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute the tf-idf matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Print the similarity matrix\n",
    "print(cosine_sim_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'VBZ', 'VBG', 'IN', 'IN', 'NN', '.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with open('pos_tags_dataset_duplicate.txt','r') as f:\n",
    "#     Lines = f.readlines()\n",
    "\n",
    "#     # print(Lines[0])\n",
    "#     lst = []\n",
    "#     for i in range(len(Lines)):\n",
    "#         lst.append(list(Lines[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('anlp-assign')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e354bad413bab38fe3bbeda77f9d500bf301a7ca6b46dcc1d81c4ff022f2634c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
