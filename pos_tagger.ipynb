{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import stanza\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = pd.read_csv('data/bad_word.txt',header=None)\n",
    "bad_words_set = set(bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/news_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments = pd.read_csv('data/comments_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63374"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63374 [00:00<?, ?it/s]/tmp/ipykernel_71509/2735669482.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_comments['comment'][i] = ' '.join(cleaned_comments['comment'][i].split()[:20])\n",
      "100%|██████████| 63374/63374 [00:16<00:00, 3850.63it/s]\n"
     ]
    }
   ],
   "source": [
    "#Truncate the comments to 20 words\n",
    "for i in tqdm(range(len(cleaned_comments))):\n",
    "    cleaned_comments['comment'][i] = ' '.join(cleaned_comments['comment'][i].split()[:20])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['cleaned_comment'] = cleaned_comments['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 14:22:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 200kB [00:00, 4.11MB/s]                    \n",
      "2023-04-24 14:22:26 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-04-24 14:22:26 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2023-04-24 14:22:26 WARNING: GPU requested, but is not available!\n",
      "2023-04-24 14:22:26 INFO: Using device: cpu\n",
      "2023-04-24 14:22:26 INFO: Loading: tokenize\n",
      "2023-04-24 14:22:26 INFO: Loading: pos\n",
      "2023-04-24 14:22:27 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "pos_tagger = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos',use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment</th>\n",
       "      <th>cleaned_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65689</th>\n",
       "      <td>65689</td>\n",
       "      <td>They already have enough local staff to fill t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65690</th>\n",
       "      <td>65690</td>\n",
       "      <td>Indeed it is! It still is ridiculously below t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65691</th>\n",
       "      <td>65691</td>\n",
       "      <td>Amazon did because it's their play for DoD con...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65692</th>\n",
       "      <td>65692</td>\n",
       "      <td>Thanks, I didn't get past the paywall.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65693</th>\n",
       "      <td>65693</td>\n",
       "      <td>This is what happens. They build and set up, g...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            comment  \\\n",
       "65689  65689  They already have enough local staff to fill t...   \n",
       "65690  65690  Indeed it is! It still is ridiculously below t...   \n",
       "65691  65691  Amazon did because it's their play for DoD con...   \n",
       "65692  65692             Thanks, I didn't get past the paywall.   \n",
       "65693  65693  This is what happens. They build and set up, g...   \n",
       "\n",
       "      cleaned_comment  \n",
       "65689             NaN  \n",
       "65690             NaN  \n",
       "65691             NaN  \n",
       "65692             NaN  \n",
       "65693             NaN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 63374/65694 [2:52:10<06:18,  6.13it/s]  \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "input should be either str, list or Document",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/pos_tagger.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/pos_tagger.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pos_tags_dataset \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/pos_tagger.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m tqdm(dataset\u001b[39m.\u001b[39miloc[:,\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtolist()):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/pos_tagger.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     doc \u001b[39m=\u001b[39m pos_tagger(sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/pos_tagger.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pos_tags_dataset\u001b[39m.\u001b[39mappend([word\u001b[39m.\u001b[39mxpos \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msentences \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sent\u001b[39m.\u001b[39mwords])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/pos_tagger.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     words\u001b[39m.\u001b[39mappend([word\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msentences \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sent\u001b[39m.\u001b[39mwords])\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544/lib/python3.9/site-packages/stanza/pipeline/core.py:464\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc, processors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(doc, processors)\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544/lib/python3.9/site-packages/stanza/pipeline/core.py:391\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, doc, processors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    380\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m    Run the pipeline\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m        will fail in some unusual manner or just have really bad results\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39many\u001b[39m([\u001b[39misinstance\u001b[39m(doc, \u001b[39mstr\u001b[39m), \u001b[39misinstance\u001b[39m(doc, \u001b[39mlist\u001b[39m),\n\u001b[1;32m    392\u001b[0m                 \u001b[39misinstance\u001b[39m(doc, Document)]), \u001b[39m'\u001b[39m\u001b[39minput should be either str, list or Document\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    394\u001b[0m     \u001b[39m# determine whether we are in bulk processing mode for multiple documents\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     bulk\u001b[39m=\u001b[39m(\u001b[39misinstance\u001b[39m(doc, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(doc) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(doc[\u001b[39m0\u001b[39m], Document))\n",
      "\u001b[0;31mAssertionError\u001b[0m: input should be either str, list or Document"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "pos_tags_dataset = []\n",
    "for sentence in tqdm(dataset.iloc[:,2].values.tolist()):\n",
    "    doc = pos_tagger(sentence)\n",
    "    pos_tags_dataset.append([word.xpos for sent in doc.sentences for word in sent.words])\n",
    "    words.append([word.text for sent in doc.sentences for word in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all rows with empty cleamed comments\n",
    "dataset = dataset[dataset['cleaned_comment'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "labels= []\n",
    "word_indices=[]\n",
    "for i,sentence in enumerate(dataset['cleaned_comment']):\n",
    "    #capture bw indices for each sentence\n",
    "    try:\n",
    "        for j,word in enumerate(sentence.split()):\n",
    "            if word.lower() in bad_words_set:\n",
    "                pos_tags_dataset[i][j] = 'BW'\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71509/3001157243.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['pos_tag'] = pos_tags_dataset\n"
     ]
    }
   ],
   "source": [
    "dataset['pos_tag'] = pos_tags_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('data/pos_tags_dataset_bw.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci544",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
