{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f07c6eda570>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lucene\n",
    "\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lucene.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = pd.read_csv('/Project/data/bad_word.txt', header=None)\n",
    "# bad_words = pd.read_csv('data/bad_word.txt', header=None)\n",
    "bad_words_set = set(bad_words[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_list= bad_words.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jigsaw_dataset = pd.read_csv('data/jigsaw_preprocessed_labels.csv',header=0)\n",
    "# jigsaw_labels = jigsaw_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.read_csv('/Project/data/pos_tags_dataset.csv',header=0)\n",
    "# news_dataset = pd.read_csv('data/pos_tags_dataset.csv',header=0)\n",
    "# labels = pd.read_csv('data/comments_model1_outputs.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ['a', 'lot', 'of', 'the', 'clients', 'i', 'wor...\n",
       "1        ['remember', 'when', 'jim', 'cramer', 'a', 'mo...\n",
       "2        ['the', 'fdic', 'only', 'insures', 'up', 'to',...\n",
       "3        ['this', 'is', 'a', 'pretty', 'big', 'deal', '...\n",
       "4        ['if', 'anyone', 's', 'wondering', 'how', 'a',...\n",
       "                               ...                        \n",
       "65689    ['they', 'already', 'have', 'enough', 'local',...\n",
       "65690    ['indeed', 'it', 'is', 'it', 'still', 'is', 'r...\n",
       "65691    ['amazon', 'did', 'because', 'it', 'is', 'thei...\n",
       "65692    ['thanks', 'i', 'did', 'not', 'get', 'past', '...\n",
       "65693    ['this', 'is', 'what', 'happens', 'they', 'bui...\n",
       "Name: words, Length: 65694, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_ignore = pd.read_csv('/Project/data/rows_to_ignore.txt',header=None)\n",
    "# rows_to_ignore = pd.read_csv('data/rows_to_ignore.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = news_dataset.drop(news_dataset.index[rows_to_ignore[0].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65616/65616 [00:27<00:00, 2363.94it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "pattern = r\"\\b(\" + \"|\".join(bad_words_list) + r\")\\b\"\n",
    "regex = re.compile(pattern)\n",
    "\n",
    "for sentence in tqdm(news_dataset['cleaned_comments']):\n",
    "    match = regex.findall(sentence.lower())\n",
    "    labels.append(1) if match else labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.to_csv('/Project/data/pos_tags_labels_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences = news_dataset[news_dataset['label']==1]\n",
    "good_sentences = news_dataset[news_dataset['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_sentences.to_csv('/Project/data/good_sentences.csv')\n",
    "good_sentences.to_csv('/Project/data/good_sentences.csv')\n",
    "bad_sentences.to_csv('/Project/data/bad_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sent = bad_sentences[['cleaned_comments','label', 'pos_tag']]\n",
    "good_sent = good_sentences[['cleaned_comments','label', 'pos_tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_comments = []\n",
    "good_comments = []\n",
    "for bad_rows in bad_sent.itertuples(index = True):\n",
    "    bad_comments.append(bad_rows[1])\n",
    "\n",
    "for good_rows in good_sent.itertuples(index = True):\n",
    "    good_comments.append(good_rows[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete indexdir in data before running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, StringField, TextField\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, DirectoryReader\n",
    "from org.apache.lucene.search import IndexSearcher,ScoreDoc, TopDocs\n",
    "from org.apache.lucene.queries.mlt import MoreLikeThisQuery\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "import java\n",
    "import os\n",
    "\n",
    "# Indexing\n",
    "index_dir = '/Project/data/indexdir'\n",
    "# analyzer = StandardAnalyzer()\n",
    "analyzer = EnglishAnalyzer()\n",
    "\n",
    "directory = FSDirectory.open(java.nio.file.Paths.get(index_dir))\n",
    "config = IndexWriterConfig(analyzer)\n",
    "writer = IndexWriter(directory, config)\n",
    "\n",
    "if not os.path.exists(index_dir):\n",
    "    print('Created dir')\n",
    "    os.mkdir(index_dir)\n",
    "    doc1 = Document()\n",
    "    doc1.add(TextField(\"content\", \"The quick brown fox jumped over the lazy dog\",  Field.Store.YES))\n",
    "    doc1.add(TextField(\"pos_tag\", \"DT NN VB\",  Field.Store.YES))\n",
    "    writer.addDocument(doc1)\n",
    "    \n",
    "else:\n",
    "    # Insert all good sentences to segments\n",
    "    for index,good_sentence in good_sentences.iterrows():\n",
    "        doc = Document()\n",
    "        doc.add(TextField(\"content\", good_sentence['cleaned_comments'], Field.Store.YES))\n",
    "        doc.add(TextField(\"pos_tag\", good_sentence['pos_tag'],  Field.Store.YES))\n",
    "        writer.addDocument(doc)\n",
    "\n",
    "# writer.commit()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24190/24190 [00:23<00:00, 1041.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Searching\n",
    "searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "analyzer = EnglishAnalyzer()\n",
    "final_similar_sentences = []\n",
    "final_pos_tags = []\n",
    "# Get MoreLikeThisQuery for the first document\n",
    "# more_like_this = MoreLikeThisQuery(\"dude my employer uses svb we are dead in the water right now\",[\"content\"],analyzer,'content')\n",
    "for bad_sentence in tqdm(bad_sentences['cleaned_comments']):\n",
    "    # print(f' BS: {bad_sentence}')\n",
    "    similar_sentences=[]\n",
    "    pos_tags_similar_sentences=[]\n",
    "    pos_tags_bad_sentences=[]\n",
    "    more_like_this = MoreLikeThisQuery(bad_sentence,[\"content\"],analyzer,\"content\")\n",
    "    hits = searcher.search(more_like_this,10).scoreDocs\n",
    "    # print results\n",
    "    # print(f\"Found {len(hits)} hits:\")\n",
    "    for hit in hits:\n",
    "        doc = searcher.doc(hit.doc)\n",
    "        # print(f\"{doc['content']}   score: {hit.score}\")\n",
    "        similar_sentences.append(doc['content'])\n",
    "        pos_tags_similar_sentences.append(doc['pos_tag'])\n",
    "    final_similar_sentences.append(similar_sentences)\n",
    "    final_pos_tags.append(pos_tags_similar_sentences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences_similar = pd.DataFrame({'bad_sentences':bad_sentences['cleaned_comments'],'bad_sentences_pos_tags':bad_sentences['pos_tag'],'similar_sentences':final_similar_sentences,'pos_tags_similar_sentences':final_pos_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences_similar.to_csv('/Project/data/bad_sentences_similar_pos_tags.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i think there is a real possibility that over the long term this ruins stein s career ',\n",
       " 'it was based on the real life rampart division in the lapd iirc ',\n",
       " 'that is an ap standards thing not a bias thing ',\n",
       " 'this one is different as his is based on selling his company to twitter and having wages as part of the compensation package not a normal employment scenario ',\n",
       " 'really awful title and the article is not much better yikes ap',\n",
       " 'you know he was seeing the movie planet of the apes in that story right ',\n",
       " 'how would the ap know they simply report what the agency says ',\n",
       " 'ruin their childhood to own the libs',\n",
       " 'almost all of the cops in we own this city are based named after real people they even feature their real [ mugshots ] c c e a e adfeb jumbo x _ap jpg in the intro ',\n",
       " 'great violent people with ruined lives ']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_similar_sentences[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'based and real these apes ruin a lot of shit for the normal ones '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_sentences['cleaned_comments'].iloc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7564c074f31accd314b76e75d557053f2db515971302aea3335cc481b5057884"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
