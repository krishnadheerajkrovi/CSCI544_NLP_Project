{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "JVM is already running and updating its classpath failed. Call initVM() instead just once but with a classpath keyword argument set to the module.CLASSPATH strings of all the JCC extension modules to be imported by this process",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlucene\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m lucene\u001b[39m.\u001b[39;49minitVM()\n",
      "\u001b[0;31mValueError\u001b[0m: JVM is already running and updating its classpath failed. Call initVM() instead just once but with a classpath keyword argument set to the module.CLASSPATH strings of all the JCC extension modules to be imported by this process"
     ]
    }
   ],
   "source": [
    "import lucene\n",
    "\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (2.0.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m784.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /root/.local/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.65.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = pd.read_csv('/Project/data/bad_word.txt', header=None)\n",
    "# bad_words = pd.read_csv('data/bad_word.txt', header=None)\n",
    "bad_words_set = set(bad_words[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_list= bad_words.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.read_csv('/Project/data/pos_tags_dataset_bw.csv',header=0)\n",
    "# news_dataset = pd.read_csv('data/pos_tags_dataset.csv',header=0)\n",
    "# labels = pd.read_csv('data/comments_model1_outputs.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ['a', 'lot', 'of', 'the', 'clients', 'i', 'wor...\n",
       "1        ['remember', 'when', 'jim', 'cramer', 'a', 'mo...\n",
       "2        ['the', 'fdic', 'only', 'insures', 'up', 'to',...\n",
       "3        ['this', 'is', 'a', 'pretty', 'big', 'deal', '...\n",
       "4        ['if', 'anyone', 's', 'wondering', 'how', 'a',...\n",
       "                               ...                        \n",
       "65689    ['they', 'already', 'have', 'enough', 'local',...\n",
       "65690    ['indeed', 'it', 'is', 'it', 'still', 'is', 'r...\n",
       "65691    ['amazon', 'did', 'because', 'it', 'is', 'thei...\n",
       "65692    ['thanks', 'i', 'did', 'not', 'get', 'past', '...\n",
       "65693    ['this', 'is', 'what', 'happens', 'they', 'bui...\n",
       "Name: words, Length: 65694, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_ignore = pd.read_csv('/Project/data/rows_to_ignore.txt',header=None)\n",
    "# rows_to_ignore = pd.read_csv('data/rows_to_ignore.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = news_dataset.drop(news_dataset.index[rows_to_ignore[0].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65616/65616 [00:27<00:00, 2424.36it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "pattern = r\"\\b(\" + \"|\".join(bad_words_list) + r\")\\b\"\n",
    "regex = re.compile(pattern)\n",
    "\n",
    "for sentence in tqdm(news_dataset['cleaned_comments']):\n",
    "    match = regex.findall(sentence.lower())\n",
    "    labels.append(1) if match else labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.to_csv('/Project/data/pos_tags_labels_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences = news_dataset[news_dataset['label']==1]\n",
    "good_sentences = news_dataset[news_dataset['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_sentences.to_csv('/Project/data/good_sentences.csv')\n",
    "good_sentences.to_csv('/Project/data/good_sentences.csv')\n",
    "bad_sentences.to_csv('/Project/data/bad_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sent = bad_sentences[['cleaned_comments','label', 'pos_tag']]\n",
    "good_sent = good_sentences[['cleaned_comments','label', 'pos_tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_comments = []\n",
    "good_comments = []\n",
    "for bad_rows in bad_sent.itertuples(index = True):\n",
    "    bad_comments.append(bad_rows[1])\n",
    "\n",
    "for good_rows in good_sent.itertuples(index = True):\n",
    "    good_comments.append(good_rows[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete indexdir in data before running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, StringField, TextField\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, DirectoryReader\n",
    "from org.apache.lucene.search import IndexSearcher,ScoreDoc, TopDocs\n",
    "from org.apache.lucene.queries.mlt import MoreLikeThisQuery\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "import java\n",
    "import os\n",
    "\n",
    "# Indexing\n",
    "index_dir = '/Project/data/indexdir'\n",
    "# analyzer = StandardAnalyzer()\n",
    "analyzer = EnglishAnalyzer()\n",
    "\n",
    "directory = FSDirectory.open(java.nio.file.Paths.get(index_dir))\n",
    "config = IndexWriterConfig(analyzer)\n",
    "writer = IndexWriter(directory, config)\n",
    "\n",
    "if not os.path.exists(index_dir):\n",
    "    print('Created dir')\n",
    "    os.mkdir(index_dir)\n",
    "    doc1 = Document()\n",
    "    doc1.add(TextField(\"content\", \"The quick brown fox jumped over the lazy dog\",  Field.Store.YES))\n",
    "    doc1.add(TextField(\"pos_tag\", \"DT NN VB\",  Field.Store.YES))\n",
    "    writer.addDocument(doc1)\n",
    "    \n",
    "else:\n",
    "    # Insert all good sentences to segments\n",
    "    for index,good_sentence in good_sentences.iterrows():\n",
    "        doc = Document()\n",
    "        doc.add(TextField(\"content\", good_sentence['cleaned_comments'], Field.Store.YES))\n",
    "        doc.add(TextField(\"pos_tag\", good_sentence['pos_tag'],  Field.Store.YES))\n",
    "        writer.addDocument(doc)\n",
    "\n",
    "# writer.commit()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24190/24190 [00:21<00:00, 1143.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Searching\n",
    "searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "analyzer = EnglishAnalyzer()\n",
    "final_similar_sentences = []\n",
    "final_pos_tags = []\n",
    "# Get MoreLikeThisQuery for the first document\n",
    "# more_like_this = MoreLikeThisQuery(\"dude my employer uses svb we are dead in the water right now\",[\"content\"],analyzer,'content')\n",
    "for bad_sentence in tqdm(bad_sentences['cleaned_comments']):\n",
    "    # print(f' BS: {bad_sentence}')\n",
    "    similar_sentences=[]\n",
    "    pos_tags_similar_sentences=[]\n",
    "    pos_tags_bad_sentences=[]\n",
    "    more_like_this = MoreLikeThisQuery(bad_sentence,[\"content\"],analyzer,\"content\")\n",
    "    hits = searcher.search(more_like_this,10).scoreDocs\n",
    "    # print results\n",
    "    # print(f\"Found {len(hits)} hits:\")\n",
    "    for hit in hits:\n",
    "        doc = searcher.doc(hit.doc)\n",
    "        # print(f\"{doc['content']}   score: {hit.score}\")\n",
    "        similar_sentences.append(doc['content'])\n",
    "        pos_tags_similar_sentences.append(doc['pos_tag'])\n",
    "    final_similar_sentences.append(similar_sentences)\n",
    "    final_pos_tags.append(pos_tags_similar_sentences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences_similar = pd.DataFrame({'bad_sentences':bad_sentences['cleaned_comments'],'bad_sentences_pos_tags':bad_sentences['pos_tag'],'similar_sentences':final_similar_sentences,'pos_tags_similar_sentences':final_pos_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences_similar.to_csv('/Project/data/bad_sentences_similar_pos_tags.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i think there is a real possibility that over the long term this ruins stein s career ',\n",
       " 'it was based on the real life rampart division in the lapd iirc ',\n",
       " 'that is an ap standards thing not a bias thing ',\n",
       " 'this one is different as his is based on selling his company to twitter and having wages as part of the compensation package not a normal employment scenario ',\n",
       " 'really awful title and the article is not much better yikes ap',\n",
       " 'you know he was seeing the movie planet of the apes in that story right ',\n",
       " 'how would the ap know they simply report what the agency says ',\n",
       " 'ruin their childhood to own the libs',\n",
       " 'almost all of the cops in we own this city are based named after real people they even feature their real [ mugshots ] c c e a e adfeb jumbo x _ap jpg in the intro ',\n",
       " 'great violent people with ruined lives ']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_similar_sentences[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'based and real these apes ruin a lot of shit for the normal ones '"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_sentences['cleaned_comments'].iloc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7564c074f31accd314b76e75d557053f2db515971302aea3335cc481b5057884"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
