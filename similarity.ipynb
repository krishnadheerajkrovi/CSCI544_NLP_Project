{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f8fac841430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lucene\n",
    "\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /root/.local/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.21.0\n",
      "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /root/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, tqdm, numpy, pandas\n",
      "Successfully installed numpy-1.24.3 pandas-2.0.1 pytz-2023.3 tqdm-4.65.0 tzdata-2023.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = pd.read_csv('/Project/data/bad_word.txt', header=None)\n",
    "# bad_words = pd.read_csv('data/bad_word.txt', header=None)\n",
    "bad_words_set = set(bad_words[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_list= bad_words.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.read_csv('/Project/data/pos_tags_dataset_bw.csv',header=0)\n",
    "# news_dataset = pd.read_csv('data/pos_tags_dataset.csv',header=0)\n",
    "# labels = pd.read_csv('data/comments_model1_outputs.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment</th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A lot of the clients I work with use SVB. This...</td>\n",
       "      <td>a lot of the clients i work with use svb this ...</td>\n",
       "      <td>['DT', 'NN', 'IN', 'DT', 'NNS', 'PRP', 'VBP', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Remember when Jim Cramer a month ago said on h...</td>\n",
       "      <td>remember when jim cramer a month ago said on h...</td>\n",
       "      <td>['VB', 'WRB', 'NNP', 'NNP', 'DT', 'NN', 'RB', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The FDIC only insures up to $250k, what happen...</td>\n",
       "      <td>the fdic only insures up to k what happens to ...</td>\n",
       "      <td>['DT', 'NN', 'RB', 'VBZ', 'RP', 'TO', 'NN', 'W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>This is a pretty big deal , they had a huge am...</td>\n",
       "      <td>this is a pretty big deal they had a huge amou...</td>\n",
       "      <td>['DT', 'VBZ', 'DT', 'RB', 'JJ', 'NN', 'PRP', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>If anyone's wondering how a bank with 200+ bil...</td>\n",
       "      <td>if anyones wondering how a bank with billion d...</td>\n",
       "      <td>['IN', 'PRP', 'VBG', 'WRB', 'DT', 'NN', 'IN', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            comment   \n",
       "0      0  A lot of the clients I work with use SVB. This...  \\\n",
       "1      1  Remember when Jim Cramer a month ago said on h...   \n",
       "2      2  The FDIC only insures up to $250k, what happen...   \n",
       "3      3  This is a pretty big deal , they had a huge am...   \n",
       "4      4  If anyone's wondering how a bank with 200+ bil...   \n",
       "\n",
       "                                     cleaned_comment   \n",
       "0  a lot of the clients i work with use svb this ...  \\\n",
       "1  remember when jim cramer a month ago said on h...   \n",
       "2  the fdic only insures up to k what happens to ...   \n",
       "3  this is a pretty big deal they had a huge amou...   \n",
       "4  if anyones wondering how a bank with billion d...   \n",
       "\n",
       "                                             pos_tag  \n",
       "0  ['DT', 'NN', 'IN', 'DT', 'NNS', 'PRP', 'VBP', ...  \n",
       "1  ['VB', 'WRB', 'NNP', 'NNP', 'DT', 'NN', 'RB', ...  \n",
       "2  ['DT', 'NN', 'RB', 'VBZ', 'RP', 'TO', 'NN', 'W...  \n",
       "3  ['DT', 'VBZ', 'DT', 'RB', 'JJ', 'NN', 'PRP', '...  \n",
       "4  ['IN', 'PRP', 'VBG', 'WRB', 'DT', 'NN', 'IN', ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63374/63374 [00:11<00:00, 5371.21it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "pattern = r\"\\b(\" + \"|\".join(bad_words_list) + r\")\\b\"\n",
    "regex = re.compile(pattern)\n",
    "\n",
    "for sentence in tqdm(news_dataset['cleaned_comment']):\n",
    "    match = regex.findall(sentence.lower())\n",
    "    labels.append(1) if match else labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.to_csv('/Project/data/pos_tags_labels_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences = news_dataset[news_dataset['label']==1]\n",
    "good_sentences = news_dataset[news_dataset['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_sentences.to_csv('/Project/data/good_sentences.csv')\n",
    "good_sentences.to_csv('/Project/data/good_sentences.csv')\n",
    "bad_sentences.to_csv('/Project/data/bad_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sent = bad_sentences[['cleaned_comment','label', 'pos_tag']]\n",
    "good_sent = good_sentences[['cleaned_comment','label', 'pos_tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_comments = []\n",
    "good_comments = []\n",
    "for bad_rows in bad_sent.itertuples(index = True):\n",
    "    bad_comments.append(bad_rows[1])\n",
    "\n",
    "for good_rows in good_sent.itertuples(index = True):\n",
    "    good_comments.append(good_rows[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete indexdir in data before running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, StringField, TextField\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, DirectoryReader\n",
    "from org.apache.lucene.search import IndexSearcher,ScoreDoc, TopDocs\n",
    "from org.apache.lucene.queries.mlt import MoreLikeThisQuery\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "import java\n",
    "import os\n",
    "\n",
    "# Indexing\n",
    "index_dir = '/Project/data/indexdir'\n",
    "# analyzer = StandardAnalyzer()\n",
    "analyzer = EnglishAnalyzer()\n",
    "\n",
    "directory = FSDirectory.open(java.nio.file.Paths.get(index_dir))\n",
    "config = IndexWriterConfig(analyzer)\n",
    "writer = IndexWriter(directory, config)\n",
    "\n",
    "if not os.path.exists(index_dir):\n",
    "    print('Created dir')\n",
    "    os.mkdir(index_dir)\n",
    "    doc1 = Document()\n",
    "    doc1.add(TextField(\"content\", \"The quick brown fox jumped over the lazy dog\",  Field.Store.YES))\n",
    "    doc1.add(TextField(\"pos_tag\", \"DT NN VB\",  Field.Store.YES))\n",
    "    writer.addDocument(doc1)\n",
    "    \n",
    "else:\n",
    "    # Insert all good sentences to segments\n",
    "    for index,good_sentence in good_sentences.iterrows():\n",
    "        doc = Document()\n",
    "        doc.add(TextField(\"content\", good_sentence['cleaned_comment'], Field.Store.YES))\n",
    "        doc.add(TextField(\"pos_tag\", good_sentence['pos_tag'],  Field.Store.YES))\n",
    "        writer.addDocument(doc)\n",
    "\n",
    "# writer.commit()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16764/16764 [00:13<00:00, 1212.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Searching\n",
    "searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "analyzer = EnglishAnalyzer()\n",
    "final_similar_sentences = []\n",
    "final_pos_tags = []\n",
    "# Get MoreLikeThisQuery for the first document\n",
    "# more_like_this = MoreLikeThisQuery(\"dude my employer uses svb we are dead in the water right now\",[\"content\"],analyzer,'content')\n",
    "for bad_sentence in tqdm(bad_sentences['cleaned_comment']):\n",
    "    # print(f' BS: {bad_sentence}')\n",
    "    similar_sentences=[]\n",
    "    pos_tags_similar_sentences=[]\n",
    "    pos_tags_bad_sentences=[]\n",
    "    more_like_this = MoreLikeThisQuery(bad_sentence,[\"content\"],analyzer,\"content\")\n",
    "    hits = searcher.search(more_like_this,10).scoreDocs\n",
    "    # print results\n",
    "    # print(f\"Found {len(hits)} hits:\")\n",
    "    for hit in hits:\n",
    "        doc = searcher.doc(hit.doc)\n",
    "        # print(f\"{doc['content']}   score: {hit.score}\")\n",
    "        similar_sentences.append(doc['content'])\n",
    "        pos_tags_similar_sentences.append(doc['pos_tag'])\n",
    "    final_similar_sentences.append(similar_sentences)\n",
    "    final_pos_tags.append(pos_tags_similar_sentences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences_similar = pd.DataFrame({'bad_sentences':bad_sentences['cleaned_comment'],'bad_sentences_pos_tags':bad_sentences['pos_tag'],'similar_sentences':final_similar_sentences,'pos_tags_similar_sentences':final_pos_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences_similar.to_csv('/Project/data/bad_sentences_similar_pos_tags.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summer in canada is just less snow',\n",
       " 'isnt dozens it is dozen',\n",
       " 'i think i know which city will become the top destination this summer',\n",
       " 'with a group of a dozen strangers less than three hours to decide lunch is pretty impressive',\n",
       " 'there are dozens of us',\n",
       " 'will probably continue wearing tops yeah but the chance at least increases besides some men are less picky about the',\n",
       " 'ive known the law here in austin allows women to go top less i saw a woman at a park',\n",
       " 'this is the biggest part of the problem',\n",
       " 'hope it lasts until summer',\n",
       " 'the biggest gang in la is the lapd']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_similar_sentences[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my biggest issue with our top less women in summer is that they look like they have a dozen tits'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_sentences['cleaned_comment'].iloc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7564c074f31accd314b76e75d557053f2db515971302aea3335cc481b5057884"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
