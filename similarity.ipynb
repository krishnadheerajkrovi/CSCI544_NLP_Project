{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucene.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = pd.read_csv('/Project/data/bad_word.txt', header=None)\n",
    "bad_words_set = set(bad_words[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jigsaw_dataset = pd.read_csv('data/jigsaw_preprocessed_labels.csv',header=0)\n",
    "# jigsaw_labels = jigsaw_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.read_csv('/Project/data/pos_tags_dataset.csv',header=0)\n",
    "# labels = pd.read_csv('data/comments_model1_outputs.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_ignore = pd.read_csv('/Project/data/rows_to_ignore.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = news_dataset.drop(news_dataset.index[rows_to_ignore[0].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= []\n",
    "word_indices=[]\n",
    "for sentence in news_dataset['cleaned_comments']:    \n",
    "    #capture bw indices for each sentence\n",
    "    flag = False\n",
    "    try:\n",
    "        for word in sentence.split():\n",
    "            if word.lower() in bad_words_set:\n",
    "                flag = True\n",
    "        labels.append(1) if flag else labels.append(0)\n",
    "    except:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_dataset.to_csv('/Project/data/pos_tags_labels_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sentences = news_dataset[news_dataset['label']==1]\n",
    "good_sentences = news_dataset[news_dataset['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sent = bad_sentences[['cleaned_comments','label', 'pos_tag']]\n",
    "good_sent = good_sentences[['cleaned_comments','label', 'pos_tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_comments = []\n",
    "good_comments = []\n",
    "for bad_rows in bad_sent.itertuples(index = True):\n",
    "    bad_comments.append(bad_rows[1])\n",
    "\n",
    "for good_rows in good_sent.itertuples(index = True):\n",
    "    good_comments.append(good_rows[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, StringField, TextField\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, DirectoryReader\n",
    "from org.apache.lucene.search import IndexSearcher,ScoreDoc, TopDocs\n",
    "from org.apache.lucene.queries.mlt import MoreLikeThisQuery\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "import java\n",
    "import os\n",
    "\n",
    "# Indexing\n",
    "index_dir = '/Project/data/indexdir'\n",
    "# analyzer = StandardAnalyzer()\n",
    "analyzer = EnglishAnalyzer()\n",
    "\n",
    "directory = FSDirectory.open(java.nio.file.Paths.get(index_dir))\n",
    "config = IndexWriterConfig(analyzer)\n",
    "writer = IndexWriter(directory, config)\n",
    "\n",
    "if not os.path.exists(index_dir):\n",
    "    print('Created dir')\n",
    "    os.mkdir(index_dir)\n",
    "    doc1 = Document()\n",
    "    doc1.add(Field(\"content\", \"The quick brown fox jumped over the lazy dog\",  TextField.TYPE_STORED))\n",
    "    writer.addDocument(doc1)\n",
    "    \n",
    "else:\n",
    "    # Insert all good sentences to segments\n",
    "    for good_sentence in good_sentences['cleaned_comments']:\n",
    "        doc = Document()\n",
    "        doc.add(TextField(\"content\", good_sentence, Field.Store.YES))\n",
    "        writer.addDocument(doc)\n",
    "\n",
    "# writer.commit()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24051/24051 [00:22<00:00, 1060.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Searching\n",
    "searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "analyzer = EnglishAnalyzer()\n",
    "final_similar_sentences = []\n",
    "# Get MoreLikeThisQuery for the first document\n",
    "# more_like_this = MoreLikeThisQuery(\"dude my employer uses svb we are dead in the water right now\",[\"content\"],analyzer,'content')\n",
    "for bad_sentence in tqdm(bad_sentences['cleaned_comments']):\n",
    "    # print(f' BS: {bad_sentence}')\n",
    "    similar_sentences=[]\n",
    "    more_like_this = MoreLikeThisQuery(bad_sentence,[\"content\"],analyzer,\"content\")\n",
    "    hits = searcher.search(more_like_this,10).scoreDocs\n",
    "    # print results\n",
    "    # print(f\"Found {len(hits)} hits:\")\n",
    "    for hit in hits:\n",
    "        doc = searcher.doc(hit.doc)\n",
    "        # print(f\"{doc['content']}   score: {hit.score}\")\n",
    "        similar_sentences.append(doc['content'])\n",
    "    final_similar_sentences.append(similar_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can cancers get cancers ',\n",
       " 'those responsible for the killings have been sacked ',\n",
       " 'yes i have two cancers at hte same time',\n",
       " ' bans entry syria security agent killings video ',\n",
       " 'us is sitting at around reported deaths so for we racked up about deaths by covid tracking in was slow but only showing about deaths i personally think it is much underreported was about deaths for the us ',\n",
       " 'care home deaths ',\n",
       " 'were the two guns that were used in the killings ever recovered ',\n",
       " 'within cells interlinked ',\n",
       " ' to make fun of the cell phone guy',\n",
       " 'do you really need more rosd deaths']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_similar_sentences[954]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that does not describe the situation right at all it is primarily due to inflammation forcing more cell death and division and all three because it obesity fucks with other things too like hormone production signaling to the body in other ways to divide cells more smoking and alcohol also because direct cell death alcohol is used to denature and kill and we drink it every cell killed as you swallow it down into your mouth down your throat through your stomach intestines and so on not even including where else the body takes some of it through your blood even it is increasing the odds of cancer everywhere it passes over we are not constantly battling cancer it is not lying in wait to pounce or constantly assaulting us it is more like we are unknowingly doing things that may let a trojan horse of former cells now undying cells loose from somewhere '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_sentences['cleaned_comments'].iloc[954]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7564c074f31accd314b76e75d557053f2db515971302aea3335cc481b5057884"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
