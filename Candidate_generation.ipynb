{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2088c25b",
   "metadata": {},
   "source": [
    "Distiled Roberta tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a7f3a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 20920, 232, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "tokenizer(\"Hello world\")['input_ids']\n",
    "tokenizer(\" Hello world\")['input_ids']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32121e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_5_tokens: [7105, 17835, 26536, 11141, 39072]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForMaSkedLM\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained('distilroberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "sequence = f\"What the {tokenizer.mask_token} is going on?\" # \"The world will end in <mask>\"\n",
    "\n",
    "input_seq = tokenizer.encode(sequence, return_tensors='pt') # tensor([[0, 133, 232, 40, 253, 11, 50264, 2]])\n",
    "mask_token_index = torch.where(input_seq == tokenizer.mask_token_id)[1] # (tensor([0]), tensor([6])) - we only want the the 2nd dimension\n",
    "token_logits = model(input_seq).logits\n",
    "masked_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(masked_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "# print('sequence:', sequence)\n",
    "# print('input_seq:', input_seq)\n",
    "# print('mask_token_index:', mask_token_index)\n",
    "# print('token_logits:', token_logits)\n",
    "# print('masked_token_logits:', masked_token_logits)\n",
    "print('top_5_tokens:', top_5_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63dc1bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What the  hell is going on?\n",
      "What the  heck is going on?\n",
      "What the  fuck is going on?\n",
      "What the  Hell is going on?\n",
      "What the  HELL is going on?\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "Sk = [1,2,4,5,6]\n",
    "Wk = ['A','B','C','D']\n",
    "if len(Wk)<len(Sk):\n",
    "    perms = list(permutations(Sk,len(Wk)))\n",
    "else:\n",
    "    perms = list(permutations(Wk,len(Sk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6fe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09443594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def candidate_set_generation(xi, pi0, Ti, T0i, F):\n",
    "    # Initialize candidate sentence with masked tokens\n",
    "    c0 = '[MASK]' * len(pi0)\n",
    "    Ci = {c0}\n",
    "    # Find shared POS tags between pi and pi0\n",
    "    Tshared = Ti.intersection(T0i)\n",
    "    for tk in Tshared:\n",
    "        # Find words in xi that are tagged with tk\n",
    "        Wk = [word for word, pos in xi if pos == tk]\n",
    "        # Find positions in pi0 where tk occurs\n",
    "        Sk = [i for i, pos in enumerate(pi0) if pos == tk]\n",
    "        # Generate all possible assignments of words in Wk to positions in Sk\n",
    "        Ak = list(permutations(Wk, len(Sk)))\n",
    "        if not Ak:\n",
    "            continue\n",
    "        # Generate new candidate sentences by filling in masked tokens with word assignments\n",
    "        new_Ci = set()\n",
    "        for cj in Ci:\n",
    "            for a in Ak:\n",
    "                c0j = [c for c in cj]\n",
    "                for i, word in zip(Sk, a):\n",
    "                    c0j[i] = word\n",
    "                new_Ci.add(' '.join(c0j))\n",
    "        Ci = new_Ci\n",
    "    # Apply mask-filling model to each candidate sentence and return the set of filled sentences\n",
    "    return {F(cj) for cj in Ci}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "model = RobertaForMaskedLM.from_pretrained('distilroberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define mask-filling function using RoBERTa\n",
    "def fill_mask(text):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    # Find the [MASK] token index\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "    # Get model predictions for the masked tokens\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "        print(output)\n",
    "        mask_token_logits = output.logits[0, mask_token_index, :]\n",
    "    # Get the top predicted tokens and convert them to strings\n",
    "    top_tokens = torch.topk(mask_token_logits, k=5, dim=1).indices[0].tolist()\n",
    "    top_token_strings = tokenizer.convert_ids_to_tokens(top_tokens)\n",
    "    # Replace the [MASK] token with each of the top predicted tokens and return the resulting texts\n",
    "    # print(top_token_strings)\n",
    "    return [text[:mask_token_index[0].item()] + token + text[mask_token_index[0].item()+1:] for token in top_token_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb91819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLMOutput(loss=None, logits=tensor([[[ 3.4195e+01, -4.1623e+00,  2.0543e+01,  ...,  4.6145e-01,\n",
      "           3.5389e+00,  1.2653e+01],\n",
      "         [ 1.3564e+00, -5.3182e+00,  1.0158e+01,  ..., -6.0494e+00,\n",
      "          -4.8035e+00,  2.2738e+00],\n",
      "         [-2.5319e+00, -6.0481e+00,  8.3431e+00,  ..., -7.0483e+00,\n",
      "          -5.6234e+00,  9.2884e-02],\n",
      "         ...,\n",
      "         [ 3.4954e-01, -4.4015e+00,  1.4322e+01,  ..., -3.1012e+00,\n",
      "          -2.6098e+00, -3.2432e-02],\n",
      "         [-1.0425e+00, -4.7949e+00,  1.4543e+01,  ..., -3.0679e+00,\n",
      "          -1.2790e+00, -2.6544e+00],\n",
      "         [ 1.5625e+01, -5.6885e+00,  2.0755e+01,  ..., -3.3742e+00,\n",
      "          -8.6062e-01,  4.5708e+00]]]), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m T0i \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mDT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJJ\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNN\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Call candidate_set_generation function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m candidate_sentences \u001b[39m=\u001b[39m candidate_set_generation(xi, pi, pi0, Vr, Ti, T0i, fill_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Print the set of candidate sentences\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(candidate_sentences)\n",
      "\u001b[1;32m/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb Cell 3\u001b[0m in \u001b[0;36mcandidate_set_generation\u001b[0;34m(xi, pi, pi0, Vr, Ti, T0i, F)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     Ci \u001b[39m=\u001b[39m new_Ci\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Apply mask-filling model to each candidate sentence and return the set of filled sentences\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m {F(cj) \u001b[39mfor\u001b[39;00m cj \u001b[39min\u001b[39;00m Ci}\n",
      "\u001b[1;32m/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb Cell 3\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     Ci \u001b[39m=\u001b[39m new_Ci\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Apply mask-filling model to each candidate sentence and return the set of filled sentences\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m {F(cj) \u001b[39mfor\u001b[39;00m cj \u001b[39min\u001b[39;00m Ci}\n",
      "\u001b[1;32m/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb Cell 3\u001b[0m in \u001b[0;36mfill_mask\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     mask_token_logits \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m, mask_token_index, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Get the top predicted tokens and convert them to strings\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m top_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtopk(mask_token_logits, k\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mindices[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m top_token_strings \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(top_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kd/Documents/USC/CSCI544/Project/Code/CSCI544_NLP_Project/Candidate_generation.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Replace the [MASK] token with each of the top predicted tokens and return the resulting texts\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Example inputs\n",
    "xi = [('The', 'DT'), ('fuck', '[MASK]'), ('brown', 'JJ'), ('fox', 'NN')]\n",
    "pi = ['The', 'quick', 'brown', 'fox']\n",
    "pi0 = ['DT', 'JJ', 'JJ', 'NN']\n",
    "Vr = {'brown': 1}\n",
    "Ti = {'DT', 'JJ', 'NN'}\n",
    "T0i = {'DT', 'JJ', 'NN'}\n",
    "\n",
    "# Call candidate_set_generation function\n",
    "candidate_sentences = candidate_set_generation(xi, pi, pi0, Vr, Ti, T0i, fill_mask)\n",
    "\n",
    "# Print the set of candidate sentences\n",
    "print(candidate_sentences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ab1c051",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "https://www.kaggle.com/code/juliusalphonso/filling-in-masked-words-with-roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a25a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci544",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
