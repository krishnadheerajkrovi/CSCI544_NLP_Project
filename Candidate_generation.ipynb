{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def candidate_set_generation(xi, pi0, Ti, T0i, F):\n",
    "    # Initialize candidate sentence with masked tokens\n",
    "    c0 = '[MASK]' * len(pi0)\n",
    "    Ci = {c0}\n",
    "    # Find shared POS tags between pi and pi0\n",
    "    Tshared = Ti.intersection(T0i)\n",
    "    for tk in Tshared:\n",
    "        # Find words in xi that are tagged with tk\n",
    "        Wk = [word for word, pos in xi if pos == tk]\n",
    "        # Find positions in pi0 where tk occurs\n",
    "        Sk = [i for i, pos in enumerate(pi0) if pos == tk]\n",
    "        # Generate all possible assignments of words in Wk to positions in Sk\n",
    "        Ak = list(permutations(Wk, len(Sk)))\n",
    "        if not Ak:\n",
    "            continue\n",
    "        # Generate new candidate sentences by filling in masked tokens with word assignments\n",
    "        new_Ci = set()\n",
    "        for cj in Ci:\n",
    "            for a in Ak:\n",
    "                c0j = [c for c in cj]\n",
    "                for i, word in zip(Sk, a):\n",
    "                    c0j[i] = word\n",
    "                new_Ci.add(' '.join(c0j))\n",
    "        Ci = new_Ci\n",
    "    # Apply mask-filling model to each candidate sentence and return the set of filled sentences\n",
    "    return {F(cj) for cj in Ci}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ee7a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I like     d o g s       f u c k i n g       S       K       ]       [       M       A       S       K       ]       [       M       A       S       K       ]       [       M       A       S       K       ]       [       M       A       S       K       ]',\n",
       " 'I like     f u c k i n g       d o g s       S       K       ]       [       M       A       S       K       ]       [       M       A       S       K       ]       [       M       A       S       K       ]       [       M       A       S       K       ]'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi = [('I', 'PRP'), ('like', 'VBP'), ('fucking', 'NNS'), ('and', 'CC'), ('dogs', 'NNS')]\n",
    "pi0 = ['PRP', 'VBP', 'NNS', 'CC', 'NNS']\n",
    "Ti = {'PRP', 'VBP', 'NNS'}\n",
    "T0i = {'PRP', 'VBP', 'NNS', 'CC'}\n",
    "F = \n",
    "\n",
    "candidate_set_generation(xi, pi0, Ti, T0i, F)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2088c25b",
   "metadata": {},
   "source": [
    "Distiled Roberta tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "tokenizer(\"Hello world\")['input_ids']\n",
    "tokenizer(\" Hello world\")['input_ids']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32121e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForMaSkedLM\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained('distilroberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "sequence = f\"What the {tokenizer.mask_token} is going on?\" # \"The world will end in <mask>\"\n",
    "\n",
    "input_seq = tokenizer.encode(sequence, return_tensors='pt') # tensor([[0, 133, 232, 40, 253, 11, 50264, 2]])\n",
    "mask_token_index = torch.where(input_seq == tokenizer.mask_token_id)[1] # (tensor([0]), tensor([6])) - we only want the the 2nd dimension\n",
    "token_logits = model(input_seq).logits\n",
    "masked_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(masked_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "# print('sequence:', sequence)\n",
    "# print('input_seq:', input_seq)\n",
    "# print('mask_token_index:', mask_token_index)\n",
    "# print('token_logits:', token_logits)\n",
    "# print('masked_token_logits:', masked_token_logits)\n",
    "print('top_5_tokens:', top_5_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "Sk = [1,2,4,5]\n",
    "Wk = ['A','B','C']\n",
    "if len(Wk)<len(Sk):\n",
    "    Ak = list(permutations(Sk,len(Wk)))\n",
    "else:\n",
    "    Ak = list(permutations(Wk,len(Sk)))\n",
    "print(Ak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae99936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "Sk = [1,2]\n",
    "Wk = ['A','B','C']\n",
    "if len(Wk)<len(Sk):\n",
    "    Ak = list(permutations(Sk,len(Wk)))\n",
    "else:\n",
    "    Ak = list(permutations(Wk,len(Sk)))\n",
    "print(Ak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64520fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = '[MASK]' * 4\n",
    "Ci = {c0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f61ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Ci = set()\n",
    "for cj in Ci:\n",
    "    for a in Ak:\n",
    "        c0j = [c for c in cj]\n",
    "        for i, word in zip(Sk, a):\n",
    "            c0j[i] = word\n",
    "        new_Ci.add(' '.join(c0j))\n",
    "Ci = new_Ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "model = RobertaForMaskedLM.from_pretrained('distilroberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define mask-filling function using RoBERTa\n",
    "def fill_mask(text):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    # Find the [MASK] token index\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "    # Get model predictions for the masked tokens\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "        print(output)\n",
    "        mask_token_logits = output.logits[0, mask_token_index, :]\n",
    "    # Get the top predicted tokens and convert them to strings\n",
    "    top_tokens = torch.topk(mask_token_logits, k=5, dim=1).indices[0].tolist()\n",
    "    top_token_strings = tokenizer.convert_ids_to_tokens(top_tokens)\n",
    "    # Replace the [MASK] token with each of the top predicted tokens and return the resulting texts\n",
    "    # print(top_token_strings)\n",
    "    return [text[:mask_token_index[0].item()] + token + text[mask_token_index[0].item()+1:] for token in top_token_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09443594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def candidate_set_generation(xi, pi0, Ti, T0i, F):\n",
    "    # Initialize candidate sentence with masked tokens\n",
    "    c0 = '[MASK]' * len(pi0)\n",
    "    Ci = {c0}\n",
    "    # Find shared POS tags between pi and pi0\n",
    "    Tshared = Ti.intersection(T0i)\n",
    "    print(f'T shared {Tshared}')\n",
    "    for tk in Tshared:\n",
    "        print(tk)\n",
    "        # Find words in xi that are tagged with tk\n",
    "        Wk = [word for word, pos in xi if pos == tk]\n",
    "        print(f'Wk {Wk}')\n",
    "        # Find positions in pi0 where tk occurs\n",
    "        # for i, pos in enumerate(pi0):\n",
    "        #     print(pos[1],tk)\n",
    "        Sk = [i for i, pos in enumerate(pi0) if pos[1] == tk]\n",
    "        print(f'Sk {Sk}')\n",
    "        # Generate all possible assignments of words in Wk to positions in Sk\n",
    "        Ak = list(permutations(Wk, len(Sk)))\n",
    "        print(f'Ak {Ak}')\n",
    "        if not Ak:\n",
    "            continue\n",
    "        # Generate new candidate sentences by filling in masked tokens with word assignments\n",
    "        new_Ci = set()\n",
    "        for cj in Ci:\n",
    "            for a in Ak:\n",
    "                c0j = [c for c in cj]\n",
    "                for i, word in zip(Sk, a):\n",
    "                    c0j[i] = word\n",
    "                new_Ci.add(' '.join(c0j))\n",
    "        Ci = new_Ci\n",
    "    # Apply mask-filling model to each candidate sentence and return the set of filled sentences\n",
    "    print(len(Ci))\n",
    "    return Ci\n",
    "    # return {F(cj) for cj in Ci}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9811a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = 'i get this feeling that derailments happen a lot and we are only paying attention now because of what happened in ohio that said why the fuck do they happen so often'\n",
    "pi0 = 'it is not they happen all the time there are thousands of train derailments a year but when a particularly bad one happens news outlets jump on the trend and start reporting more they are not more frequent this is just the first a lot of people are hearing about them edit you all can downvote but i am right look it up if you are really concerned you should know this'\n",
    "tag_pi0 = ['PRP', 'VBZ', 'RB', 'PRP', 'VBP', 'PDT', 'DT', 'NN', 'EX', 'VBP', 'NNS', 'IN', 'NN', 'NNS', 'DT', 'NN', 'CC', 'WRB', 'DT', 'RB', 'JJ', 'NN', 'VBZ', 'NN', 'NNS', 'VB', 'IN', 'DT', 'NN', 'CC', 'VBP', 'VBG', 'JJR', 'PRP', 'VBP', 'RB', 'RBR', 'JJ', 'DT', 'VBZ', 'RB', 'DT', 'JJ', 'DT', 'NN', 'IN', 'NNS', 'VBP', 'VBG', 'IN', 'PRP', 'NN', 'PRP', 'DT', 'MD', 'VB', 'CC', 'PRP', 'VBP', 'JJ', 'VB', 'PRP', 'RP', 'IN', 'PRP', 'VBP', 'RB', 'JJ', 'PRP', 'MD', 'VB', 'DT']\n",
    "tag_xi = ['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', 'VBP', 'DT', 'NN', 'CC', 'PRP', 'VBP', 'RB', 'VBG', 'NN', 'RB', 'IN', 'IN', 'WP', 'VBD', 'IN', 'NNP', 'WDT', 'VBD', 'WRB', 'DT', 'BW', 'VBP', 'PRP', 'VB', 'RB', 'RB']\n",
    "T0i = set(['PRP', 'VBZ', 'RB', 'PRP', 'VBP', 'PDT', 'DT', 'NN', 'EX', 'VBP', 'NNS', 'IN', 'NN', 'NNS', 'DT', 'NN', 'CC', 'WRB', 'DT', 'RB', 'JJ', 'NN', 'VBZ', 'NN', 'NNS', 'VB', 'IN', 'DT', 'NN', 'CC', 'VBP', 'VBG', 'JJR', 'PRP', 'VBP', 'RB', 'RBR', 'JJ', 'DT', 'VBZ', 'RB', 'DT', 'JJ', 'DT', 'NN', 'IN', 'NNS', 'VBP', 'VBG', 'IN', 'PRP', 'NN', 'PRP', 'DT', 'MD', 'VB', 'CC', 'PRP', 'VBP', 'JJ', 'VB', 'PRP', 'RP', 'IN', 'PRP', 'VBP', 'RB', 'JJ', 'PRP', 'MD', 'VB', 'DT'])\n",
    "Ti = set(['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNS', 'VBP', 'DT', 'NN', 'CC', 'PRP', 'VBP', 'RB', 'VBG', 'NN', 'RB', 'IN', 'IN', 'WP', 'VBD', 'IN', 'NNP', 'WDT', 'VBD', 'WRB', 'DT', 'BW', 'VBP', 'PRP', 'VB', 'RB', 'RB'])\n",
    "xi_mod = [(word,tag) for word,tag in zip(xi.split(),tag_xi)]\n",
    "pi0_mod = [(word,tag) for word,tag in zip(pi0.split(),tag_pi0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95339ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T shared {'RB', 'IN', 'VB', 'DT', 'NNS', 'VBG', 'WRB', 'PRP', 'CC', 'VBP', 'NN'}\n",
      "RB\n",
      "Wk ['only', 'now', 'so', 'often']\n",
      "Sk [2, 19, 35, 40, 66]\n",
      "Ak []\n",
      "IN\n",
      "Wk ['that', 'because', 'of', 'in']\n",
      "Sk [11, 26, 45, 49, 63]\n",
      "Ak []\n",
      "VB\n",
      "Wk ['happen']\n",
      "Sk [25, 55, 60, 70]\n",
      "Ak []\n",
      "DT\n",
      "Wk ['this', 'a', 'the']\n",
      "Sk [6, 14, 18, 27, 38, 41, 43, 53, 71]\n",
      "Ak []\n",
      "NNS\n",
      "Wk ['derailments']\n",
      "Sk [10, 13, 24, 46]\n",
      "Ak []\n",
      "VBG\n",
      "Wk ['paying']\n",
      "Sk [31, 48]\n",
      "Ak []\n",
      "WRB\n",
      "Wk ['why']\n",
      "Sk [17]\n",
      "Ak [('why',)]\n",
      "PRP\n",
      "Wk ['i', 'we', 'they']\n",
      "Sk [0, 3, 33, 50, 52, 57, 61, 64, 68]\n",
      "Ak []\n",
      "CC\n",
      "Wk ['and']\n",
      "Sk [16, 29, 56]\n",
      "Ak []\n",
      "VBP\n",
      "Wk ['get', 'happen', 'are', 'do']\n",
      "Sk [4, 9, 30, 34, 47, 58, 65]\n",
      "Ak []\n",
      "NN\n",
      "Wk ['feeling', 'lot', 'attention']\n",
      "Sk [7, 12, 15, 21, 23, 28, 44, 51]\n",
      "Ak []\n",
      "1\n",
      "{'[ M A S K ] [ M A S K ] [ M A S K why [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ] [ M A S K ]'}\n"
     ]
    }
   ],
   "source": [
    "print(candidate_set_generation(xi=xi_mod,pi0=pi0_mod,Ti=Ti,T0i=T0i,F=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inputs\n",
    "xi = [('The', 'DT'), ('fuck', '[MASK]'), ('brown', 'JJ'), ('fox', 'NN')]\n",
    "pi = ['The', 'quick', 'brown', 'fox']\n",
    "pi0 = ['DT', 'JJ', 'JJ', 'NN']\n",
    "Vr = {'brown': 1}\n",
    "Ti = {'DT', 'JJ', 'NN'}\n",
    "T0i = {'DT', 'JJ', 'NN'}\n",
    "\n",
    "# Call candidate_set_generation function\n",
    "candidate_sentences = candidate_set_generation(xi, pi, pi0, Vr, Ti, T0i, fill_mask)\n",
    "\n",
    "# Print the set of candidate sentences\n",
    "print(candidate_sentences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ab1c051",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "https://www.kaggle.com/code/juliusalphonso/filling-in-masked-words-with-roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a25a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci544",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
